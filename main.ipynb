{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50833046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed59494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 17 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Grade10              20000 non-null  float64\n",
      " 1   Grade12              20000 non-null  float64\n",
      " 2   CPI                  20000 non-null  float64\n",
      " 3   Backlogs             20000 non-null  int64  \n",
      " 4   CodeforcesRating     20000 non-null  int64  \n",
      " 5   CompetitiveExamRank  20000 non-null  int64  \n",
      " 6   Projects             20000 non-null  int64  \n",
      " 7   Internships          20000 non-null  int64  \n",
      " 8   ExperienceMonths     20000 non-null  int64  \n",
      " 9   CollegeTier          20000 non-null  object \n",
      " 10  CommunicationSkill   20000 non-null  float64\n",
      " 11  Leadership           20000 non-null  float64\n",
      " 12  Branch               20000 non-null  object \n",
      " 13  HackathonWins        20000 non-null  int64  \n",
      " 14  ResearchPapers       20000 non-null  int64  \n",
      " 15  OpenSourceContrib    20000 non-null  int64  \n",
      " 16  PackageLPA           20000 non-null  float64\n",
      "dtypes: float64(6), int64(9), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_excel(\"placement_dataset_skewed.xlsx\")\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30fa396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade10</th>\n",
       "      <th>Grade12</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Backlogs</th>\n",
       "      <th>CodeforcesRating</th>\n",
       "      <th>CompetitiveExamRank</th>\n",
       "      <th>Projects</th>\n",
       "      <th>Internships</th>\n",
       "      <th>ExperienceMonths</th>\n",
       "      <th>CollegeTier</th>\n",
       "      <th>CommunicationSkill</th>\n",
       "      <th>Leadership</th>\n",
       "      <th>Branch</th>\n",
       "      <th>HackathonWins</th>\n",
       "      <th>ResearchPapers</th>\n",
       "      <th>OpenSourceContrib</th>\n",
       "      <th>PackageLPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.46</td>\n",
       "      <td>70.36</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1</td>\n",
       "      <td>718</td>\n",
       "      <td>11338</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>ME</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.75</td>\n",
       "      <td>95.48</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0</td>\n",
       "      <td>1177</td>\n",
       "      <td>1959</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Tier 2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>IT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.64</td>\n",
       "      <td>76.17</td>\n",
       "      <td>7.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1052</td>\n",
       "      <td>13020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Tier 2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.2</td>\n",
       "      <td>CHEM</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.41</td>\n",
       "      <td>68.63</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0</td>\n",
       "      <td>1022</td>\n",
       "      <td>3648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>CIVIL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.84</td>\n",
       "      <td>78.08</td>\n",
       "      <td>6.03</td>\n",
       "      <td>1</td>\n",
       "      <td>1315</td>\n",
       "      <td>5120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>ECE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Grade10  Grade12   CPI  Backlogs  CodeforcesRating  CompetitiveExamRank  \\\n",
       "0    73.46    70.36  6.63         1               718                11338   \n",
       "1    76.75    95.48  8.30         0              1177                 1959   \n",
       "2    76.64    76.17  7.12         1              1052                13020   \n",
       "3    76.41    68.63  6.85         0              1022                 3648   \n",
       "4    73.84    78.08  6.03         1              1315                 5120   \n",
       "\n",
       "   Projects  Internships  ExperienceMonths CollegeTier  CommunicationSkill  \\\n",
       "0         2            0                 5      Tier 3                 5.5   \n",
       "1         3            1                 6      Tier 2                 6.4   \n",
       "2         2            1                 0      Tier 2                 6.4   \n",
       "3         1            1                 3      Tier 3                 7.1   \n",
       "4         1            0                 4      Tier 3                 6.3   \n",
       "\n",
       "   Leadership Branch  HackathonWins  ResearchPapers  OpenSourceContrib  \\\n",
       "0         4.5     ME              0               0                  0   \n",
       "1         7.7     IT              0               0                  1   \n",
       "2         6.2   CHEM              1               0                  0   \n",
       "3         7.4  CIVIL              1               0                  0   \n",
       "4         7.4    ECE              0               0                  2   \n",
       "\n",
       "   PackageLPA  \n",
       "0       13.33  \n",
       "1       24.70  \n",
       "2       19.16  \n",
       "3       17.93  \n",
       "4       19.97  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ddbd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categories in training data for Branch: ['ME' 'IT' 'CHEM' 'CIVIL' 'ECE' 'CSE' 'DS' 'AI' 'EE' 'BIO']\n",
      "All categories in training data for CollegeTier: ['Tier 3' 'Tier 2' 'Tier 1']\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['Branch', 'CollegeTier']\n",
    "for col in cat_cols:\n",
    "    print(f\"All categories in training data for {col}: {df[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5165720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 17 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Grade10              20000 non-null  float64\n",
      " 1   Grade12              20000 non-null  float64\n",
      " 2   CPI                  20000 non-null  float64\n",
      " 3   Backlogs             20000 non-null  int64  \n",
      " 4   CodeforcesRating     20000 non-null  int64  \n",
      " 5   CompetitiveExamRank  20000 non-null  int64  \n",
      " 6   Projects             20000 non-null  int64  \n",
      " 7   Internships          20000 non-null  int64  \n",
      " 8   ExperienceMonths     20000 non-null  int64  \n",
      " 9   CollegeTier          20000 non-null  object \n",
      " 10  CommunicationSkill   20000 non-null  float64\n",
      " 11  Leadership           20000 non-null  float64\n",
      " 12  Branch               20000 non-null  object \n",
      " 13  HackathonWins        20000 non-null  int64  \n",
      " 14  ResearchPapers       20000 non-null  int64  \n",
      " 15  OpenSourceContrib    20000 non-null  int64  \n",
      " 16  PackageLPA           20000 non-null  float64\n",
      "dtypes: float64(6), int64(9), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c2f690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ME', 'IT', 'CHEM', 'CIVIL', 'ECE', 'CSE', 'DS', 'AI', 'EE', 'BIO'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Branch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "061b47a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tier 3', 'Tier 2', 'Tier 1'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CollegeTier'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b408147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder=OneHotEncoder(drop=None,sparse_output=False)\n",
    "cat_cols=['Branch','CollegeTier']\n",
    "encoded=encoder.fit_transform(df[cat_cols])\n",
    "encoded_df=pd.DataFrame(encoded,columns=encoder.get_feature_names_out(cat_cols))\n",
    "df_new=pd.concat([df.drop(columns=cat_cols),encoded_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a58869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch_AI</th>\n",
       "      <th>Branch_BIO</th>\n",
       "      <th>Branch_CHEM</th>\n",
       "      <th>Branch_CIVIL</th>\n",
       "      <th>Branch_CSE</th>\n",
       "      <th>Branch_DS</th>\n",
       "      <th>Branch_ECE</th>\n",
       "      <th>Branch_EE</th>\n",
       "      <th>Branch_IT</th>\n",
       "      <th>Branch_ME</th>\n",
       "      <th>CollegeTier_Tier 1</th>\n",
       "      <th>CollegeTier_Tier 2</th>\n",
       "      <th>CollegeTier_Tier 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Branch_AI  Branch_BIO  Branch_CHEM  Branch_CIVIL  Branch_CSE  Branch_DS  \\\n",
       "0        0.0         0.0          0.0           0.0         0.0        0.0   \n",
       "1        0.0         0.0          0.0           0.0         0.0        0.0   \n",
       "2        0.0         0.0          1.0           0.0         0.0        0.0   \n",
       "3        0.0         0.0          0.0           1.0         0.0        0.0   \n",
       "4        0.0         0.0          0.0           0.0         0.0        0.0   \n",
       "\n",
       "   Branch_ECE  Branch_EE  Branch_IT  Branch_ME  CollegeTier_Tier 1  \\\n",
       "0         0.0        0.0        0.0        1.0                 0.0   \n",
       "1         0.0        0.0        1.0        0.0                 0.0   \n",
       "2         0.0        0.0        0.0        0.0                 0.0   \n",
       "3         0.0        0.0        0.0        0.0                 0.0   \n",
       "4         1.0        0.0        0.0        0.0                 0.0   \n",
       "\n",
       "   CollegeTier_Tier 2  CollegeTier_Tier 3  \n",
       "0                 0.0                 1.0  \n",
       "1                 1.0                 0.0  \n",
       "2                 1.0                 0.0  \n",
       "3                 0.0                 1.0  \n",
       "4                 0.0                 1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f248ecbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade10</th>\n",
       "      <th>Grade12</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Backlogs</th>\n",
       "      <th>CodeforcesRating</th>\n",
       "      <th>CompetitiveExamRank</th>\n",
       "      <th>Projects</th>\n",
       "      <th>Internships</th>\n",
       "      <th>ExperienceMonths</th>\n",
       "      <th>CommunicationSkill</th>\n",
       "      <th>...</th>\n",
       "      <th>Branch_CIVIL</th>\n",
       "      <th>Branch_CSE</th>\n",
       "      <th>Branch_DS</th>\n",
       "      <th>Branch_ECE</th>\n",
       "      <th>Branch_EE</th>\n",
       "      <th>Branch_IT</th>\n",
       "      <th>Branch_ME</th>\n",
       "      <th>CollegeTier_Tier 1</th>\n",
       "      <th>CollegeTier_Tier 2</th>\n",
       "      <th>CollegeTier_Tier 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.46</td>\n",
       "      <td>70.36</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1</td>\n",
       "      <td>718</td>\n",
       "      <td>11338</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.75</td>\n",
       "      <td>95.48</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0</td>\n",
       "      <td>1177</td>\n",
       "      <td>1959</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.64</td>\n",
       "      <td>76.17</td>\n",
       "      <td>7.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1052</td>\n",
       "      <td>13020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.41</td>\n",
       "      <td>68.63</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0</td>\n",
       "      <td>1022</td>\n",
       "      <td>3648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.84</td>\n",
       "      <td>78.08</td>\n",
       "      <td>6.03</td>\n",
       "      <td>1</td>\n",
       "      <td>1315</td>\n",
       "      <td>5120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>70.99</td>\n",
       "      <td>75.74</td>\n",
       "      <td>6.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1183</td>\n",
       "      <td>1906</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>79.60</td>\n",
       "      <td>93.84</td>\n",
       "      <td>6.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1212</td>\n",
       "      <td>3510</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>72.92</td>\n",
       "      <td>70.32</td>\n",
       "      <td>7.38</td>\n",
       "      <td>3</td>\n",
       "      <td>772</td>\n",
       "      <td>1852</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>79.69</td>\n",
       "      <td>74.19</td>\n",
       "      <td>7.27</td>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>14885</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>70.09</td>\n",
       "      <td>85.86</td>\n",
       "      <td>7.08</td>\n",
       "      <td>0</td>\n",
       "      <td>1252</td>\n",
       "      <td>3919</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Grade10  Grade12   CPI  Backlogs  CodeforcesRating  \\\n",
       "0        73.46    70.36  6.63         1               718   \n",
       "1        76.75    95.48  8.30         0              1177   \n",
       "2        76.64    76.17  7.12         1              1052   \n",
       "3        76.41    68.63  6.85         0              1022   \n",
       "4        73.84    78.08  6.03         1              1315   \n",
       "...        ...      ...   ...       ...               ...   \n",
       "19995    70.99    75.74  6.89         0              1183   \n",
       "19996    79.60    93.84  6.46         1              1212   \n",
       "19997    72.92    70.32  7.38         3               772   \n",
       "19998    79.69    74.19  7.27         1              1600   \n",
       "19999    70.09    85.86  7.08         0              1252   \n",
       "\n",
       "       CompetitiveExamRank  Projects  Internships  ExperienceMonths  \\\n",
       "0                    11338         2            0                 5   \n",
       "1                     1959         3            1                 6   \n",
       "2                    13020         2            1                 0   \n",
       "3                     3648         1            1                 3   \n",
       "4                     5120         1            0                 4   \n",
       "...                    ...       ...          ...               ...   \n",
       "19995                 1906         1            2                 9   \n",
       "19996                 3510         3            2                 0   \n",
       "19997                 1852         3            1                 0   \n",
       "19998                14885         2            1                 2   \n",
       "19999                 3919         4            0                 1   \n",
       "\n",
       "       CommunicationSkill  ...  Branch_CIVIL  Branch_CSE  Branch_DS  \\\n",
       "0                     5.5  ...           0.0         0.0        0.0   \n",
       "1                     6.4  ...           0.0         0.0        0.0   \n",
       "2                     6.4  ...           0.0         0.0        0.0   \n",
       "3                     7.1  ...           1.0         0.0        0.0   \n",
       "4                     6.3  ...           0.0         0.0        0.0   \n",
       "...                   ...  ...           ...         ...        ...   \n",
       "19995                 6.1  ...           0.0         0.0        1.0   \n",
       "19996                 5.4  ...           0.0         1.0        0.0   \n",
       "19997                 7.2  ...           0.0         0.0        0.0   \n",
       "19998                 7.9  ...           0.0         0.0        0.0   \n",
       "19999                 5.1  ...           0.0         0.0        0.0   \n",
       "\n",
       "       Branch_ECE  Branch_EE  Branch_IT  Branch_ME  CollegeTier_Tier 1  \\\n",
       "0             0.0        0.0        0.0        1.0                 0.0   \n",
       "1             0.0        0.0        1.0        0.0                 0.0   \n",
       "2             0.0        0.0        0.0        0.0                 0.0   \n",
       "3             0.0        0.0        0.0        0.0                 0.0   \n",
       "4             1.0        0.0        0.0        0.0                 0.0   \n",
       "...           ...        ...        ...        ...                 ...   \n",
       "19995         0.0        0.0        0.0        0.0                 0.0   \n",
       "19996         0.0        0.0        0.0        0.0                 0.0   \n",
       "19997         0.0        1.0        0.0        0.0                 0.0   \n",
       "19998         0.0        0.0        0.0        1.0                 0.0   \n",
       "19999         1.0        0.0        0.0        0.0                 0.0   \n",
       "\n",
       "       CollegeTier_Tier 2  CollegeTier_Tier 3  \n",
       "0                     0.0                 1.0  \n",
       "1                     1.0                 0.0  \n",
       "2                     1.0                 0.0  \n",
       "3                     0.0                 1.0  \n",
       "4                     0.0                 1.0  \n",
       "...                   ...                 ...  \n",
       "19995                 0.0                 1.0  \n",
       "19996                 0.0                 1.0  \n",
       "19997                 1.0                 0.0  \n",
       "19998                 0.0                 1.0  \n",
       "19999                 0.0                 1.0  \n",
       "\n",
       "[20000 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6770aed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Branch_AI', 'Branch_BIO', 'Branch_CHEM', 'Branch_CIVIL',\n",
       "       'Branch_CSE', 'Branch_DS', 'Branch_ECE', 'Branch_EE', 'Branch_IT',\n",
       "       'Branch_ME', 'CollegeTier_Tier 1', 'CollegeTier_Tier 2',\n",
       "       'CollegeTier_Tier 3'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.get_feature_names_out(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1635dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade10</th>\n",
       "      <th>Grade12</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Backlogs</th>\n",
       "      <th>CodeforcesRating</th>\n",
       "      <th>CompetitiveExamRank</th>\n",
       "      <th>Projects</th>\n",
       "      <th>Internships</th>\n",
       "      <th>ExperienceMonths</th>\n",
       "      <th>CommunicationSkill</th>\n",
       "      <th>...</th>\n",
       "      <th>Branch_CIVIL</th>\n",
       "      <th>Branch_CSE</th>\n",
       "      <th>Branch_DS</th>\n",
       "      <th>Branch_ECE</th>\n",
       "      <th>Branch_EE</th>\n",
       "      <th>Branch_IT</th>\n",
       "      <th>Branch_ME</th>\n",
       "      <th>CollegeTier_Tier 1</th>\n",
       "      <th>CollegeTier_Tier 2</th>\n",
       "      <th>CollegeTier_Tier 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.46</td>\n",
       "      <td>70.36</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1</td>\n",
       "      <td>718</td>\n",
       "      <td>11338</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.75</td>\n",
       "      <td>95.48</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0</td>\n",
       "      <td>1177</td>\n",
       "      <td>1959</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.64</td>\n",
       "      <td>76.17</td>\n",
       "      <td>7.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1052</td>\n",
       "      <td>13020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.41</td>\n",
       "      <td>68.63</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0</td>\n",
       "      <td>1022</td>\n",
       "      <td>3648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.84</td>\n",
       "      <td>78.08</td>\n",
       "      <td>6.03</td>\n",
       "      <td>1</td>\n",
       "      <td>1315</td>\n",
       "      <td>5120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Grade10  Grade12   CPI  Backlogs  CodeforcesRating  CompetitiveExamRank  \\\n",
       "0    73.46    70.36  6.63         1               718                11338   \n",
       "1    76.75    95.48  8.30         0              1177                 1959   \n",
       "2    76.64    76.17  7.12         1              1052                13020   \n",
       "3    76.41    68.63  6.85         0              1022                 3648   \n",
       "4    73.84    78.08  6.03         1              1315                 5120   \n",
       "\n",
       "   Projects  Internships  ExperienceMonths  CommunicationSkill  ...  \\\n",
       "0         2            0                 5                 5.5  ...   \n",
       "1         3            1                 6                 6.4  ...   \n",
       "2         2            1                 0                 6.4  ...   \n",
       "3         1            1                 3                 7.1  ...   \n",
       "4         1            0                 4                 6.3  ...   \n",
       "\n",
       "   Branch_CIVIL  Branch_CSE  Branch_DS  Branch_ECE  Branch_EE  Branch_IT  \\\n",
       "0           0.0         0.0        0.0         0.0        0.0        0.0   \n",
       "1           0.0         0.0        0.0         0.0        0.0        1.0   \n",
       "2           0.0         0.0        0.0         0.0        0.0        0.0   \n",
       "3           1.0         0.0        0.0         0.0        0.0        0.0   \n",
       "4           0.0         0.0        0.0         1.0        0.0        0.0   \n",
       "\n",
       "   Branch_ME  CollegeTier_Tier 1  CollegeTier_Tier 2  CollegeTier_Tier 3  \n",
       "0        1.0                 0.0                 0.0                 1.0  \n",
       "1        0.0                 0.0                 1.0                 0.0  \n",
       "2        0.0                 0.0                 1.0                 0.0  \n",
       "3        0.0                 0.0                 0.0                 1.0  \n",
       "4        0.0                 0.0                 0.0                 1.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00f8ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Package']=df['PackageLPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7a41e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 29 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Grade10              20000 non-null  float64\n",
      " 1   Grade12              20000 non-null  float64\n",
      " 2   CPI                  20000 non-null  float64\n",
      " 3   Backlogs             20000 non-null  int64  \n",
      " 4   CodeforcesRating     20000 non-null  int64  \n",
      " 5   CompetitiveExamRank  20000 non-null  int64  \n",
      " 6   Projects             20000 non-null  int64  \n",
      " 7   Internships          20000 non-null  int64  \n",
      " 8   ExperienceMonths     20000 non-null  int64  \n",
      " 9   CommunicationSkill   20000 non-null  float64\n",
      " 10  Leadership           20000 non-null  float64\n",
      " 11  HackathonWins        20000 non-null  int64  \n",
      " 12  ResearchPapers       20000 non-null  int64  \n",
      " 13  OpenSourceContrib    20000 non-null  int64  \n",
      " 14  PackageLPA           20000 non-null  float64\n",
      " 15  Branch_AI            20000 non-null  float64\n",
      " 16  Branch_BIO           20000 non-null  float64\n",
      " 17  Branch_CHEM          20000 non-null  float64\n",
      " 18  Branch_CIVIL         20000 non-null  float64\n",
      " 19  Branch_CSE           20000 non-null  float64\n",
      " 20  Branch_DS            20000 non-null  float64\n",
      " 21  Branch_ECE           20000 non-null  float64\n",
      " 22  Branch_EE            20000 non-null  float64\n",
      " 23  Branch_IT            20000 non-null  float64\n",
      " 24  Branch_ME            20000 non-null  float64\n",
      " 25  CollegeTier_Tier 1   20000 non-null  float64\n",
      " 26  CollegeTier_Tier 2   20000 non-null  float64\n",
      " 27  CollegeTier_Tier 3   20000 non-null  float64\n",
      " 28  Package              20000 non-null  float64\n",
      "dtypes: float64(20), int64(9)\n",
      "memory usage: 4.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c57c9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.drop('PackageLPA',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f81d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 28 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Grade10              20000 non-null  float64\n",
      " 1   Grade12              20000 non-null  float64\n",
      " 2   CPI                  20000 non-null  float64\n",
      " 3   Backlogs             20000 non-null  int64  \n",
      " 4   CodeforcesRating     20000 non-null  int64  \n",
      " 5   CompetitiveExamRank  20000 non-null  int64  \n",
      " 6   Projects             20000 non-null  int64  \n",
      " 7   Internships          20000 non-null  int64  \n",
      " 8   ExperienceMonths     20000 non-null  int64  \n",
      " 9   CommunicationSkill   20000 non-null  float64\n",
      " 10  Leadership           20000 non-null  float64\n",
      " 11  HackathonWins        20000 non-null  int64  \n",
      " 12  ResearchPapers       20000 non-null  int64  \n",
      " 13  OpenSourceContrib    20000 non-null  int64  \n",
      " 14  Branch_AI            20000 non-null  float64\n",
      " 15  Branch_BIO           20000 non-null  float64\n",
      " 16  Branch_CHEM          20000 non-null  float64\n",
      " 17  Branch_CIVIL         20000 non-null  float64\n",
      " 18  Branch_CSE           20000 non-null  float64\n",
      " 19  Branch_DS            20000 non-null  float64\n",
      " 20  Branch_ECE           20000 non-null  float64\n",
      " 21  Branch_EE            20000 non-null  float64\n",
      " 22  Branch_IT            20000 non-null  float64\n",
      " 23  Branch_ME            20000 non-null  float64\n",
      " 24  CollegeTier_Tier 1   20000 non-null  float64\n",
      " 25  CollegeTier_Tier 2   20000 non-null  float64\n",
      " 26  CollegeTier_Tier 3   20000 non-null  float64\n",
      " 27  Package              20000 non-null  float64\n",
      "dtypes: float64(19), int64(9)\n",
      "memory usage: 4.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "996cb126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade10</th>\n",
       "      <th>Grade12</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Backlogs</th>\n",
       "      <th>CodeforcesRating</th>\n",
       "      <th>CompetitiveExamRank</th>\n",
       "      <th>Projects</th>\n",
       "      <th>Internships</th>\n",
       "      <th>ExperienceMonths</th>\n",
       "      <th>CommunicationSkill</th>\n",
       "      <th>...</th>\n",
       "      <th>Branch_CIVIL</th>\n",
       "      <th>Branch_CSE</th>\n",
       "      <th>Branch_DS</th>\n",
       "      <th>Branch_ECE</th>\n",
       "      <th>Branch_EE</th>\n",
       "      <th>Branch_IT</th>\n",
       "      <th>Branch_ME</th>\n",
       "      <th>CollegeTier_Tier 1</th>\n",
       "      <th>CollegeTier_Tier 2</th>\n",
       "      <th>CollegeTier_Tier 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.46</td>\n",
       "      <td>70.36</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1</td>\n",
       "      <td>718</td>\n",
       "      <td>11338</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.75</td>\n",
       "      <td>95.48</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0</td>\n",
       "      <td>1177</td>\n",
       "      <td>1959</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.64</td>\n",
       "      <td>76.17</td>\n",
       "      <td>7.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1052</td>\n",
       "      <td>13020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.41</td>\n",
       "      <td>68.63</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0</td>\n",
       "      <td>1022</td>\n",
       "      <td>3648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.84</td>\n",
       "      <td>78.08</td>\n",
       "      <td>6.03</td>\n",
       "      <td>1</td>\n",
       "      <td>1315</td>\n",
       "      <td>5120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>70.99</td>\n",
       "      <td>75.74</td>\n",
       "      <td>6.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1183</td>\n",
       "      <td>1906</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>79.60</td>\n",
       "      <td>93.84</td>\n",
       "      <td>6.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1212</td>\n",
       "      <td>3510</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>72.92</td>\n",
       "      <td>70.32</td>\n",
       "      <td>7.38</td>\n",
       "      <td>3</td>\n",
       "      <td>772</td>\n",
       "      <td>1852</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>79.69</td>\n",
       "      <td>74.19</td>\n",
       "      <td>7.27</td>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>14885</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>70.09</td>\n",
       "      <td>85.86</td>\n",
       "      <td>7.08</td>\n",
       "      <td>0</td>\n",
       "      <td>1252</td>\n",
       "      <td>3919</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Grade10  Grade12   CPI  Backlogs  CodeforcesRating  \\\n",
       "0        73.46    70.36  6.63         1               718   \n",
       "1        76.75    95.48  8.30         0              1177   \n",
       "2        76.64    76.17  7.12         1              1052   \n",
       "3        76.41    68.63  6.85         0              1022   \n",
       "4        73.84    78.08  6.03         1              1315   \n",
       "...        ...      ...   ...       ...               ...   \n",
       "19995    70.99    75.74  6.89         0              1183   \n",
       "19996    79.60    93.84  6.46         1              1212   \n",
       "19997    72.92    70.32  7.38         3               772   \n",
       "19998    79.69    74.19  7.27         1              1600   \n",
       "19999    70.09    85.86  7.08         0              1252   \n",
       "\n",
       "       CompetitiveExamRank  Projects  Internships  ExperienceMonths  \\\n",
       "0                    11338         2            0                 5   \n",
       "1                     1959         3            1                 6   \n",
       "2                    13020         2            1                 0   \n",
       "3                     3648         1            1                 3   \n",
       "4                     5120         1            0                 4   \n",
       "...                    ...       ...          ...               ...   \n",
       "19995                 1906         1            2                 9   \n",
       "19996                 3510         3            2                 0   \n",
       "19997                 1852         3            1                 0   \n",
       "19998                14885         2            1                 2   \n",
       "19999                 3919         4            0                 1   \n",
       "\n",
       "       CommunicationSkill  ...  Branch_CIVIL  Branch_CSE  Branch_DS  \\\n",
       "0                     5.5  ...           0.0         0.0        0.0   \n",
       "1                     6.4  ...           0.0         0.0        0.0   \n",
       "2                     6.4  ...           0.0         0.0        0.0   \n",
       "3                     7.1  ...           1.0         0.0        0.0   \n",
       "4                     6.3  ...           0.0         0.0        0.0   \n",
       "...                   ...  ...           ...         ...        ...   \n",
       "19995                 6.1  ...           0.0         0.0        1.0   \n",
       "19996                 5.4  ...           0.0         1.0        0.0   \n",
       "19997                 7.2  ...           0.0         0.0        0.0   \n",
       "19998                 7.9  ...           0.0         0.0        0.0   \n",
       "19999                 5.1  ...           0.0         0.0        0.0   \n",
       "\n",
       "       Branch_ECE  Branch_EE  Branch_IT  Branch_ME  CollegeTier_Tier 1  \\\n",
       "0             0.0        0.0        0.0        1.0                 0.0   \n",
       "1             0.0        0.0        1.0        0.0                 0.0   \n",
       "2             0.0        0.0        0.0        0.0                 0.0   \n",
       "3             0.0        0.0        0.0        0.0                 0.0   \n",
       "4             1.0        0.0        0.0        0.0                 0.0   \n",
       "...           ...        ...        ...        ...                 ...   \n",
       "19995         0.0        0.0        0.0        0.0                 0.0   \n",
       "19996         0.0        0.0        0.0        0.0                 0.0   \n",
       "19997         0.0        1.0        0.0        0.0                 0.0   \n",
       "19998         0.0        0.0        0.0        1.0                 0.0   \n",
       "19999         1.0        0.0        0.0        0.0                 0.0   \n",
       "\n",
       "       CollegeTier_Tier 2  CollegeTier_Tier 3  \n",
       "0                     0.0                 1.0  \n",
       "1                     1.0                 0.0  \n",
       "2                     1.0                 0.0  \n",
       "3                     0.0                 1.0  \n",
       "4                     0.0                 1.0  \n",
       "...                   ...                 ...  \n",
       "19995                 0.0                 1.0  \n",
       "19996                 0.0                 1.0  \n",
       "19997                 1.0                 0.0  \n",
       "19998                 0.0                 1.0  \n",
       "19999                 0.0                 1.0  \n",
       "\n",
       "[20000 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df_new.iloc[:,:-1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f82e36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_new['Package']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3b505b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        13.33\n",
       "1        24.70\n",
       "2        19.16\n",
       "3        17.93\n",
       "4        19.97\n",
       "         ...  \n",
       "19995    22.77\n",
       "19996    23.05\n",
       "19997    21.75\n",
       "19998    18.08\n",
       "19999    18.71\n",
       "Name: Package, Length: 20000, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "500ee16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94245fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8475093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression(n_jobs=-1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "LRmodel=LinearRegression(n_jobs=-1)\n",
    "LRmodel.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d92ba0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor(n_jobs=-1, n_neighbors=7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;KNeighborsRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\">?<span>Documentation for KNeighborsRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsRegressor(n_jobs=-1, n_neighbors=7)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsRegressor(n_jobs=-1, n_neighbors=7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNmodel=KNeighborsRegressor(n_neighbors=7,n_jobs=-1)\n",
    "KNNmodel.fit(X_train_scaled,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0695436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_absolute_error\n",
    "LR_y_pred=LRmodel.predict(X_test_scaled)\n",
    "KNN_y_pred=KNNmodel.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce80d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for linear regression is0.9626883120960836\n",
      "R2 Score for linear regression is0.8708185838575382\n",
      "MAE for KNN is1.2946594285714286\n",
      "R2 Score for KNN is0.7622473205792245\n"
     ]
    }
   ],
   "source": [
    "print(f'MAE for linear regression is{mean_absolute_error(y_test,LR_y_pred)}')\n",
    "print(f'R2 Score for linear regression is{r2_score(y_test,LR_y_pred)}')\n",
    "print(f'MAE for KNN is{mean_absolute_error(y_test,KNN_y_pred)}')\n",
    "print(f'R2 Score for KNN is{r2_score(y_test,KNN_y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41915a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for RF is1.2486141668464712\n",
      "R2 Score for RF is0.7810429393620753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF_model=RandomForestRegressor(n_estimators=100,max_depth=10,random_state=42)\n",
    "RF_model.fit(X_train_scaled,y_train)\n",
    "RF_y_pred=RF_model.predict(X_test_scaled)\n",
    "print(f'MAE for RF is{mean_absolute_error(y_test,RF_y_pred)}')\n",
    "print(f'R2 Score for RF is{r2_score(y_test,RF_y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bead8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a467d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "100 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [        nan -0.00299135         nan -0.00109742 -0.00522796         nan\n",
      "         nan         nan         nan         nan -0.00775947         nan\n",
      "         nan -0.00208303 -0.00125329 -0.00079596 -0.00523925 -0.00590762\n",
      " -0.00582908         nan         nan -0.00529012         nan -0.00352519\n",
      "         nan         nan -0.01037417 -0.00292178 -0.00579678 -0.00784768\n",
      "         nan -0.001083   -0.00771729 -0.00269425 -0.00073458 -0.00351477\n",
      "         nan         nan -0.00186076 -0.00064913         nan -0.00874657\n",
      " -0.00088882         nan -0.00734758 -0.00076877 -0.00200676 -0.01010656\n",
      "         nan -0.01567024]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 414}\n",
      "Test MSE: 559.16\n",
      "Test R2: 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),    \n",
    "    'max_depth': randint(5, 30),            \n",
    "    'min_samples_split': randint(2, 10),     \n",
    "    'min_samples_leaf': randint(1, 10),     \n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RF_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,               # number of random combinations\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='r2',            # evaluation metric\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {mse:.2f}\")\n",
    "print(f\"Test R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee9e0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted=df_new.sort_values(by='CompetitiveRank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd35839a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Internships</th>\n",
       "      <th>Projects</th>\n",
       "      <th>CodeforcesRating</th>\n",
       "      <th>CommunicationSkill</th>\n",
       "      <th>ExperienceMonths</th>\n",
       "      <th>Age</th>\n",
       "      <th>Grade10</th>\n",
       "      <th>Grade12</th>\n",
       "      <th>Backlogs</th>\n",
       "      <th>...</th>\n",
       "      <th>Branch_CE</th>\n",
       "      <th>Branch_CSE</th>\n",
       "      <th>Branch_ECE</th>\n",
       "      <th>Branch_ME</th>\n",
       "      <th>CollegeTag_Tier1</th>\n",
       "      <th>CollegeTag_Tier2</th>\n",
       "      <th>CollegeTag_Tier3</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Package</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19427</th>\n",
       "      <td>8.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>884</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>88.4</td>\n",
       "      <td>93.9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19289</th>\n",
       "      <td>8.59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1009</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>96.4</td>\n",
       "      <td>79.7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11034</th>\n",
       "      <td>7.12</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1598</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>25</td>\n",
       "      <td>84.9</td>\n",
       "      <td>84.1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>6.60</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1669</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>83.1</td>\n",
       "      <td>72.8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>8.15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1715</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>90.4</td>\n",
       "      <td>82.6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>7.59</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1256</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "      <td>79.1</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6982</th>\n",
       "      <td>7.72</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1387</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>83.5</td>\n",
       "      <td>89.9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>7.98</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1510</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>96.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19558</th>\n",
       "      <td>5.51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1191</td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>90.1</td>\n",
       "      <td>86.5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16878</th>\n",
       "      <td>9.38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1169</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>94.5</td>\n",
       "      <td>77.3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CGPA  Internships  Projects  CodeforcesRating  CommunicationSkill  \\\n",
       "19427  8.10            2         0               884                   9   \n",
       "19289  8.59            1         1              1009                   5   \n",
       "11034  7.12            2         5              1598                   8   \n",
       "5236   6.60            2         2              1669                   9   \n",
       "717    8.15            1         2              1715                   6   \n",
       "...     ...          ...       ...               ...                 ...   \n",
       "17812  7.59            2         2              1256                   3   \n",
       "6982   7.72            2         2              1387                   6   \n",
       "10193  7.98            0         3              1510                   3   \n",
       "19558  5.51            1         2              1191                   7   \n",
       "16878  9.38            2         2              1169                   7   \n",
       "\n",
       "       ExperienceMonths  Age  Grade10  Grade12  Backlogs  ...  Branch_CE  \\\n",
       "19427                16   23     88.4     93.9         0  ...        0.0   \n",
       "19289                41   25     96.4     79.7         0  ...        0.0   \n",
       "11034                45   25     84.9     84.1         1  ...        0.0   \n",
       "5236                 26   23     83.1     72.8         2  ...        0.0   \n",
       "717                  36   24     90.4     82.6         4  ...        0.0   \n",
       "...                 ...  ...      ...      ...       ...  ...        ...   \n",
       "17812                41   20     79.1     93.6         2  ...        1.0   \n",
       "6982                 39   23     83.5     89.9         1  ...        1.0   \n",
       "10193                18   22     96.3     77.5         1  ...        1.0   \n",
       "19558                39   26     90.1     86.5         0  ...        1.0   \n",
       "16878                16   23     94.5     77.3         1  ...        1.0   \n",
       "\n",
       "       Branch_CSE  Branch_ECE  Branch_ME  CollegeTag_Tier1  CollegeTag_Tier2  \\\n",
       "19427         1.0         0.0        0.0               0.0               1.0   \n",
       "19289         1.0         0.0        0.0               1.0               0.0   \n",
       "11034         1.0         0.0        0.0               0.0               1.0   \n",
       "5236          1.0         0.0        0.0               0.0               1.0   \n",
       "717           1.0         0.0        0.0               0.0               1.0   \n",
       "...           ...         ...        ...               ...               ...   \n",
       "17812         0.0         0.0        0.0               0.0               1.0   \n",
       "6982          0.0         0.0        0.0               0.0               0.0   \n",
       "10193         0.0         0.0        0.0               0.0               1.0   \n",
       "19558         0.0         0.0        0.0               0.0               1.0   \n",
       "16878         0.0         0.0        0.0               0.0               1.0   \n",
       "\n",
       "       CollegeTag_Tier3  Gender_Female  Gender_Male  Package  \n",
       "19427               0.0            1.0          0.0    97.52  \n",
       "19289               0.0            0.0          1.0    58.57  \n",
       "11034               0.0            0.0          1.0    57.18  \n",
       "5236                0.0            0.0          1.0    68.61  \n",
       "717                 0.0            0.0          1.0     6.35  \n",
       "...                 ...            ...          ...      ...  \n",
       "17812               0.0            0.0          1.0    63.55  \n",
       "6982                1.0            0.0          1.0    40.12  \n",
       "10193               0.0            0.0          1.0    40.13  \n",
       "19558               0.0            0.0          1.0     9.50  \n",
       "16878               0.0            1.0          0.0    13.56  \n",
       "\n",
       "[20000 rows x 21 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "667cf9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1841cea6a20>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df_sorted[\"CompetitiveRank\"], df_sorted[\"Package\"], color=\"blue\", alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5da362d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for RF is26.871014825532864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(f'MSE for RF is{mean_squared_error(y_test,RF_y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4638af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open('random_forest_model.pkl', 'wb') as f:\n",
    "#    pickle.dump(best_rf, f)\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('OHE.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7185b6da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- CollegeTier\nFeature names seen at fit time, yet now missing:\n- CollegeTag\n- Gender\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 60\u001b[0m\n\u001b[0;32m     40\u001b[0m df_in \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(new_input)\n\u001b[0;32m     42\u001b[0m numeric_cols \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGrade10\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGrade12\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPackageLPA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     58\u001b[0m ]\n\u001b[1;32m---> 60\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(df_in[cat_cols])\n\u001b[0;32m     61\u001b[0m encoded_input_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoded_input, columns\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_feature_names_out(cat_cols))\n\u001b[0;32m     62\u001b[0m input_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_in[numeric_cols], encoded_input_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1024\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1023\u001b[0m }\n\u001b[1;32m-> 1024\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m   1025\u001b[0m     X,\n\u001b[0;32m   1026\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m   1027\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1028\u001b[0m     warn_on_unknown\u001b[38;5;241m=\u001b[39mwarn_on_unknown,\n\u001b[0;32m   1029\u001b[0m )\n\u001b[0;32m   1031\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:197\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    188\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m     ignore_category_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    193\u001b[0m ):\n\u001b[0;32m    194\u001b[0m     X_list, n_samples, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(\n\u001b[0;32m    195\u001b[0m         X, force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m     X_int \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n_samples, n_features), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- CollegeTier\nFeature names seen at fit time, yet now missing:\n- CollegeTag\n- Gender\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import sys,json\n",
    "\n",
    "\n",
    "#with open('random_forest_model.pkl', 'rb') as f:\n",
    " #   RF_model = pickle.load(f)\n",
    "\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "with open('OHE.pkl', 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "neinput = [\n",
    "    [7.76, 0, 3, 16459, 'ME', 757, 55, 10, 18, 'IIT', 93.6, 95, 0, 'M']\n",
    "]\n",
    "new_input = pd.DataFrame([{\n",
    "    \"Grade10\": 90,\n",
    "    \"Grade12\": 88,\n",
    "    \"CPI\": 9.0,\n",
    "    \"Backlogs\": 0,\n",
    "    \"CodeforcesRating\": 2200,\n",
    "    \"CompetitiveExamRank\": 1500,\n",
    "    \"Projects\": 5,\n",
    "    \"Internships\": 2,\n",
    "    \"ExperienceMonths\": 8,\n",
    "    \"CollegeTier\": \"Tier 1\",\n",
    "    \"CommunicationSkill\": 8.5,\n",
    "    \"Leadership\": 9,\n",
    "    \"Branch\": \"CSE\",\n",
    "    \"HackathonWins\": 1,\n",
    "    \"ResearchPapers\": 1,\n",
    "    \"OpenSourceContrib\": 4\n",
    "}])\n",
    "\n",
    "columns = ['CGPA', 'Internships', 'Projects', 'CompetitiveRank', 'Branch',\n",
    "           'CodeforcesRating', 'CommunicationSkill', 'ExperienceMonths', 'Age',\n",
    "           'CollegeTag', 'Grade10', 'Grade12', 'Backlogs', 'Gender']\n",
    "\n",
    "df_in = pd.DataFrame(new_input)\n",
    "\n",
    "numeric_cols = [\n",
    "    'Grade10',\n",
    "    'Grade12',\n",
    "    'CPI',\n",
    "    'Backlogs',\n",
    "    'CodeforcesRating',\n",
    "    'CompetitiveExamRank',\n",
    "    'Projects',\n",
    "    'Internships',\n",
    "    'ExperienceMonths',\n",
    "    'HackathonWins',\n",
    "    'ResearchPapers',\n",
    "    'OpenSourceContrib',\n",
    "    'CommunicationSkill',\n",
    "    'Leadership',\n",
    "    'PackageLPA'\n",
    "]\n",
    "\n",
    "encoded_input = encoder.transform(df_in[cat_cols])\n",
    "encoded_input_df = pd.DataFrame(encoded_input, columns=encoder.get_feature_names_out(cat_cols))\n",
    "input_combined = pd.concat([df_in[numeric_cols], encoded_input_df], axis=1)\n",
    "input_combined = scaler.transform(input_combined)\n",
    "predicted_package = RF_model.predict(input_combined)\n",
    "\n",
    "print(f\"Predicted Package: {predicted_package[0]:.2f} LPA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05376397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15b5b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sjija\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),  \n",
    "    Dense(32, activation=\"relu\"), \n",
    "    Dense(16,activation='relu'),\n",
    "    Dense(8,activation='relu'),  \n",
    "    Dense(4,activation='relu'),                       \n",
    "    Dense(1, activation=\"linear\")                          \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96e3b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",   # mean squared error for regression\n",
    "    metrics=[\"mae\"]  # mean absolute error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a54aaeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 827.4612 - mae: 21.6827 - val_loss: 600.8027 - val_mae: 20.0239\n",
      "Epoch 2/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 620.0560 - mae: 20.2132 - val_loss: 590.5329 - val_mae: 19.9549\n",
      "Epoch 3/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 606.8486 - mae: 20.0270 - val_loss: 580.8479 - val_mae: 19.6571\n",
      "Epoch 4/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 600.9204 - mae: 20.0004 - val_loss: 575.5006 - val_mae: 19.6182\n",
      "Epoch 5/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 598.3239 - mae: 19.9639 - val_loss: 580.5966 - val_mae: 19.9396\n",
      "Epoch 6/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 594.8011 - mae: 19.8859 - val_loss: 576.4227 - val_mae: 19.6290\n",
      "Epoch 7/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 592.6636 - mae: 19.8590 - val_loss: 576.5939 - val_mae: 19.9013\n",
      "Epoch 8/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 591.7248 - mae: 19.8523 - val_loss: 572.7161 - val_mae: 19.7281\n",
      "Epoch 9/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 586.6149 - mae: 19.7714 - val_loss: 567.2823 - val_mae: 19.5149\n",
      "Epoch 10/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 586.4305 - mae: 19.7832 - val_loss: 574.1586 - val_mae: 19.6345\n",
      "Epoch 11/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 583.4000 - mae: 19.7464 - val_loss: 577.4631 - val_mae: 19.5505\n",
      "Epoch 12/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 584.4240 - mae: 19.7456 - val_loss: 566.8165 - val_mae: 19.5472\n",
      "Epoch 13/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 582.3094 - mae: 19.7227 - val_loss: 575.1298 - val_mae: 19.5367\n",
      "Epoch 14/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 582.2045 - mae: 19.7174 - val_loss: 573.4379 - val_mae: 19.5279\n",
      "Epoch 15/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 582.0034 - mae: 19.7270 - val_loss: 565.6666 - val_mae: 19.4926\n",
      "Epoch 16/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 585.7682 - mae: 19.7594 - val_loss: 565.5259 - val_mae: 19.5718\n",
      "Epoch 17/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 581.6213 - mae: 19.7393 - val_loss: 564.4608 - val_mae: 19.5955\n",
      "Epoch 18/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 580.2496 - mae: 19.7001 - val_loss: 568.8988 - val_mae: 19.4908\n",
      "Epoch 19/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 581.9805 - mae: 19.7198 - val_loss: 568.3855 - val_mae: 19.5034\n",
      "Epoch 20/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 580.0666 - mae: 19.7013 - val_loss: 566.8730 - val_mae: 19.4807\n",
      "Epoch 21/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 581.7939 - mae: 19.7338 - val_loss: 578.3099 - val_mae: 19.9624\n",
      "Epoch 22/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 579.7544 - mae: 19.6903 - val_loss: 565.6502 - val_mae: 19.6173\n",
      "Epoch 23/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 579.7069 - mae: 19.6988 - val_loss: 569.4036 - val_mae: 19.7728\n",
      "Epoch 24/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 579.4461 - mae: 19.6857 - val_loss: 564.1581 - val_mae: 19.5717\n",
      "Epoch 25/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.7600 - mae: 19.6820 - val_loss: 565.7311 - val_mae: 19.4936\n",
      "Epoch 26/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 579.3619 - mae: 19.6900 - val_loss: 571.2111 - val_mae: 19.4893\n",
      "Epoch 27/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578.7725 - mae: 19.6829 - val_loss: 564.0331 - val_mae: 19.5209\n",
      "Epoch 28/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.9583 - mae: 19.6705 - val_loss: 570.3513 - val_mae: 19.4921\n",
      "Epoch 29/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.9526 - mae: 19.6685 - val_loss: 563.5197 - val_mae: 19.5284\n",
      "Epoch 30/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578.3244 - mae: 19.6849 - val_loss: 570.0405 - val_mae: 19.5000\n",
      "Epoch 31/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578.6671 - mae: 19.6726 - val_loss: 564.3577 - val_mae: 19.5590\n",
      "Epoch 32/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.7211 - mae: 19.6521 - val_loss: 565.9877 - val_mae: 19.5786\n",
      "Epoch 33/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.7817 - mae: 19.6621 - val_loss: 578.6879 - val_mae: 19.9898\n",
      "Epoch 34/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 579.1428 - mae: 19.7021 - val_loss: 564.6503 - val_mae: 19.5253\n",
      "Epoch 35/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.3896 - mae: 19.6545 - val_loss: 565.6522 - val_mae: 19.5118\n",
      "Epoch 36/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 579.9342 - mae: 19.7211 - val_loss: 563.8351 - val_mae: 19.5316\n",
      "Epoch 37/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578.5764 - mae: 19.6888 - val_loss: 571.6354 - val_mae: 19.8175\n",
      "Epoch 38/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.4286 - mae: 19.6728 - val_loss: 567.0675 - val_mae: 19.6897\n",
      "Epoch 39/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.2415 - mae: 19.6714 - val_loss: 568.2565 - val_mae: 19.4813\n",
      "Epoch 40/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578.2691 - mae: 19.6776 - val_loss: 565.0204 - val_mae: 19.6410\n",
      "Epoch 41/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.5759 - mae: 19.6566 - val_loss: 563.5179 - val_mae: 19.5593\n",
      "Epoch 42/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578.1888 - mae: 19.6948 - val_loss: 563.8235 - val_mae: 19.5037\n",
      "Epoch 43/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.9819 - mae: 19.6418 - val_loss: 564.2098 - val_mae: 19.5991\n",
      "Epoch 44/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.9134 - mae: 19.6675 - val_loss: 563.3027 - val_mae: 19.5022\n",
      "Epoch 45/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.0882 - mae: 19.6653 - val_loss: 567.7589 - val_mae: 19.5141\n",
      "Epoch 46/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578.0594 - mae: 19.6734 - val_loss: 564.9946 - val_mae: 19.6257\n",
      "Epoch 47/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578.0923 - mae: 19.6784 - val_loss: 567.9589 - val_mae: 19.7327\n",
      "Epoch 48/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.7643 - mae: 19.6498 - val_loss: 563.9730 - val_mae: 19.5081\n",
      "Epoch 49/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 577.1537 - mae: 19.6603 - val_loss: 565.2131 - val_mae: 19.4857\n",
      "Epoch 50/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.6856 - mae: 19.6540 - val_loss: 563.8779 - val_mae: 19.5424\n",
      "Epoch 51/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.6757 - mae: 19.6546 - val_loss: 564.6880 - val_mae: 19.5616\n",
      "Epoch 52/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.6625 - mae: 19.6433 - val_loss: 568.4525 - val_mae: 19.5230\n",
      "Epoch 53/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.2288 - mae: 19.6427 - val_loss: 566.9373 - val_mae: 19.6735\n",
      "Epoch 54/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.1322 - mae: 19.6568 - val_loss: 565.0203 - val_mae: 19.6393\n",
      "Epoch 55/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.3423 - mae: 19.6379 - val_loss: 574.4003 - val_mae: 19.8466\n",
      "Epoch 56/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.5228 - mae: 19.6502 - val_loss: 563.6462 - val_mae: 19.5739\n",
      "Epoch 57/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.9005 - mae: 19.6306 - val_loss: 568.0519 - val_mae: 19.7323\n",
      "Epoch 58/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.1873 - mae: 19.6559 - val_loss: 563.7658 - val_mae: 19.4976\n",
      "Epoch 59/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.1984 - mae: 19.6577 - val_loss: 563.4558 - val_mae: 19.5703\n",
      "Epoch 60/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.7289 - mae: 19.6553 - val_loss: 565.4835 - val_mae: 19.6457\n",
      "Epoch 61/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.5638 - mae: 19.6671 - val_loss: 569.5677 - val_mae: 19.7676\n",
      "Epoch 62/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.9366 - mae: 19.6264 - val_loss: 566.4859 - val_mae: 19.4897\n",
      "Epoch 63/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.0700 - mae: 19.6401 - val_loss: 587.6138 - val_mae: 20.1791\n",
      "Epoch 64/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.8766 - mae: 19.6521 - val_loss: 563.6162 - val_mae: 19.4926\n",
      "Epoch 65/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.7528 - mae: 19.6538 - val_loss: 563.8359 - val_mae: 19.4993\n",
      "Epoch 66/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.5627 - mae: 19.6321 - val_loss: 563.9855 - val_mae: 19.5772\n",
      "Epoch 67/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.0353 - mae: 19.6551 - val_loss: 563.6633 - val_mae: 19.5334\n",
      "Epoch 68/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.3158 - mae: 19.6468 - val_loss: 563.1412 - val_mae: 19.5297\n",
      "Epoch 69/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.7478 - mae: 19.6521 - val_loss: 563.5044 - val_mae: 19.5435\n",
      "Epoch 70/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.8848 - mae: 19.6323 - val_loss: 563.2513 - val_mae: 19.5544\n",
      "Epoch 71/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.4185 - mae: 19.6530 - val_loss: 566.3782 - val_mae: 19.4898\n",
      "Epoch 72/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.3145 - mae: 19.6593 - val_loss: 564.2142 - val_mae: 19.6152\n",
      "Epoch 73/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.6011 - mae: 19.6173 - val_loss: 567.3492 - val_mae: 19.4876\n",
      "Epoch 74/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.5980 - mae: 19.6541 - val_loss: 564.7485 - val_mae: 19.6275\n",
      "Epoch 75/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.7047 - mae: 19.6417 - val_loss: 568.2414 - val_mae: 19.4822\n",
      "Epoch 76/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.9717 - mae: 19.6369 - val_loss: 564.4297 - val_mae: 19.6198\n",
      "Epoch 77/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.3861 - mae: 19.6481 - val_loss: 564.0430 - val_mae: 19.4979\n",
      "Epoch 78/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.3646 - mae: 19.6419 - val_loss: 563.0658 - val_mae: 19.5333\n",
      "Epoch 79/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.0587 - mae: 19.6374 - val_loss: 568.5482 - val_mae: 19.7303\n",
      "Epoch 80/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.1509 - mae: 19.6399 - val_loss: 574.8120 - val_mae: 19.5295\n",
      "Epoch 81/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.3697 - mae: 19.6394 - val_loss: 565.7830 - val_mae: 19.6217\n",
      "Epoch 82/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.1606 - mae: 19.6511 - val_loss: 563.1033 - val_mae: 19.5521\n",
      "Epoch 83/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.5853 - mae: 19.6355 - val_loss: 566.9422 - val_mae: 19.6792\n",
      "Epoch 84/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.3412 - mae: 19.6449 - val_loss: 568.3682 - val_mae: 19.4929\n",
      "Epoch 85/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.4985 - mae: 19.6105 - val_loss: 564.4797 - val_mae: 19.5643\n",
      "Epoch 86/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.8334 - mae: 19.6406 - val_loss: 563.7090 - val_mae: 19.5725\n",
      "Epoch 87/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.8688 - mae: 19.6417 - val_loss: 563.7881 - val_mae: 19.4983\n",
      "Epoch 88/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.8232 - mae: 19.6313 - val_loss: 562.7786 - val_mae: 19.5309\n",
      "Epoch 89/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.5231 - mae: 19.6241 - val_loss: 564.0998 - val_mae: 19.5425\n",
      "Epoch 90/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.2682 - mae: 19.6532 - val_loss: 565.2168 - val_mae: 19.5253\n",
      "Epoch 91/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.7294 - mae: 19.6209 - val_loss: 564.6517 - val_mae: 19.6312\n",
      "Epoch 92/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.9786 - mae: 19.6418 - val_loss: 564.1496 - val_mae: 19.5047\n",
      "Epoch 93/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.6937 - mae: 19.6299 - val_loss: 563.5686 - val_mae: 19.5878\n",
      "Epoch 94/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.4067 - mae: 19.6241 - val_loss: 565.7160 - val_mae: 19.6446\n",
      "Epoch 95/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.4501 - mae: 19.6154 - val_loss: 570.7317 - val_mae: 19.4839\n",
      "Epoch 96/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 575.3137 - mae: 19.6618 - val_loss: 567.8505 - val_mae: 19.7180\n",
      "Epoch 97/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.8476 - mae: 19.6171 - val_loss: 564.4766 - val_mae: 19.6205\n",
      "Epoch 98/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.2392 - mae: 19.6317 - val_loss: 563.5759 - val_mae: 19.4890\n",
      "Epoch 99/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.5070 - mae: 19.6321 - val_loss: 563.9025 - val_mae: 19.4985\n",
      "Epoch 100/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 575.3265 - mae: 19.6419 - val_loss: 571.5406 - val_mae: 19.8292\n",
      "Epoch 101/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.1962 - mae: 19.6404 - val_loss: 569.4059 - val_mae: 19.4873\n",
      "Epoch 102/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 576.0415 - mae: 19.6474 - val_loss: 563.3148 - val_mae: 19.5842\n",
      "Epoch 103/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.1898 - mae: 19.6022 - val_loss: 572.9948 - val_mae: 19.8577\n",
      "Epoch 104/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.9128 - mae: 19.6165 - val_loss: 570.7975 - val_mae: 19.7734\n",
      "Epoch 105/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.4081 - mae: 19.6368 - val_loss: 564.3065 - val_mae: 19.4719\n",
      "Epoch 106/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.5209 - mae: 19.6238 - val_loss: 563.4982 - val_mae: 19.4953\n",
      "Epoch 107/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.5845 - mae: 19.6330 - val_loss: 563.4211 - val_mae: 19.5778\n",
      "Epoch 108/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.5172 - mae: 19.6347 - val_loss: 563.4127 - val_mae: 19.5704\n",
      "Epoch 109/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.6501 - mae: 19.6321 - val_loss: 563.5551 - val_mae: 19.4893\n",
      "Epoch 110/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.7537 - mae: 19.6231 - val_loss: 563.8702 - val_mae: 19.5745\n",
      "Epoch 111/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.3455 - mae: 19.6274 - val_loss: 564.1708 - val_mae: 19.5803\n",
      "Epoch 112/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.6595 - mae: 19.6369 - val_loss: 563.6561 - val_mae: 19.4862\n",
      "Epoch 113/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.3300 - mae: 19.6312 - val_loss: 565.8002 - val_mae: 19.4728\n",
      "Epoch 114/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.1254 - mae: 19.6273 - val_loss: 563.9494 - val_mae: 19.4761\n",
      "Epoch 115/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.6084 - mae: 19.6220 - val_loss: 563.4779 - val_mae: 19.5603\n",
      "Epoch 116/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.8659 - mae: 19.6296 - val_loss: 564.5779 - val_mae: 19.6398\n",
      "Epoch 117/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.8859 - mae: 19.6286 - val_loss: 564.9979 - val_mae: 19.6510\n",
      "Epoch 118/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.2044 - mae: 19.6159 - val_loss: 567.2859 - val_mae: 19.7036\n",
      "Epoch 119/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.8310 - mae: 19.6293 - val_loss: 562.9069 - val_mae: 19.5530\n",
      "Epoch 120/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.8503 - mae: 19.6423 - val_loss: 563.5914 - val_mae: 19.5512\n",
      "Epoch 121/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.7492 - mae: 19.6302 - val_loss: 564.7626 - val_mae: 19.5030\n",
      "Epoch 122/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.4590 - mae: 19.6140 - val_loss: 563.6616 - val_mae: 19.5549\n",
      "Epoch 123/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.8607 - mae: 19.6299 - val_loss: 567.5944 - val_mae: 19.4820\n",
      "Epoch 124/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.3075 - mae: 19.6355 - val_loss: 566.0150 - val_mae: 19.4974\n",
      "Epoch 125/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.1927 - mae: 19.6041 - val_loss: 577.8066 - val_mae: 19.5787\n",
      "Epoch 126/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 574.2009 - mae: 19.6269 - val_loss: 565.9036 - val_mae: 19.5002\n",
      "Epoch 127/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.3951 - mae: 19.6259 - val_loss: 565.3123 - val_mae: 19.6455\n",
      "Epoch 128/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.4957 - mae: 19.6093 - val_loss: 571.0103 - val_mae: 19.8050\n",
      "Epoch 129/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.3685 - mae: 19.6247 - val_loss: 580.6351 - val_mae: 19.5297\n",
      "Epoch 130/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.3707 - mae: 19.6167 - val_loss: 574.6119 - val_mae: 19.8946\n",
      "Epoch 131/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 572.9955 - mae: 19.6144 - val_loss: 563.1135 - val_mae: 19.5630\n",
      "Epoch 132/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.0063 - mae: 19.6106 - val_loss: 562.9609 - val_mae: 19.5266\n",
      "Epoch 133/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.1045 - mae: 19.6116 - val_loss: 565.5436 - val_mae: 19.6736\n",
      "Epoch 134/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.2867 - mae: 19.6197 - val_loss: 566.5214 - val_mae: 19.6960\n",
      "Epoch 135/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.8495 - mae: 19.6258 - val_loss: 563.7600 - val_mae: 19.6071\n",
      "Epoch 136/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.4643 - mae: 19.6128 - val_loss: 563.9213 - val_mae: 19.5787\n",
      "Epoch 137/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.3487 - mae: 19.5967 - val_loss: 563.3466 - val_mae: 19.5518\n",
      "Epoch 138/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.0294 - mae: 19.5986 - val_loss: 564.0096 - val_mae: 19.5747\n",
      "Epoch 139/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.5787 - mae: 19.6315 - val_loss: 563.2782 - val_mae: 19.5472\n",
      "Epoch 140/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.7047 - mae: 19.6256 - val_loss: 563.2476 - val_mae: 19.5216\n",
      "Epoch 141/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.3329 - mae: 19.6120 - val_loss: 562.6857 - val_mae: 19.5214\n",
      "Epoch 142/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.4952 - mae: 19.6091 - val_loss: 572.8864 - val_mae: 19.4990\n",
      "Epoch 143/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.2089 - mae: 19.6188 - val_loss: 562.7910 - val_mae: 19.5497\n",
      "Epoch 144/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.3730 - mae: 19.6263 - val_loss: 563.8854 - val_mae: 19.4902\n",
      "Epoch 145/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 572.7040 - mae: 19.6047 - val_loss: 564.1586 - val_mae: 19.5247\n",
      "Epoch 146/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 573.4639 - mae: 19.6199 - val_loss: 565.2568 - val_mae: 19.6523\n",
      "Epoch 147/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 572.9993 - mae: 19.6081 - val_loss: 563.1931 - val_mae: 19.5574\n",
      "Epoch 148/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 573.1279 - mae: 19.6067 - val_loss: 566.5665 - val_mae: 19.6892\n",
      "Epoch 149/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.5151 - mae: 19.6214 - val_loss: 564.7621 - val_mae: 19.4879\n",
      "Epoch 150/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.6185 - mae: 19.6180 - val_loss: 569.9971 - val_mae: 19.4668\n",
      "Epoch 151/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.5294 - mae: 19.6080 - val_loss: 563.8163 - val_mae: 19.5129\n",
      "Epoch 152/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.1812 - mae: 19.6061 - val_loss: 564.2504 - val_mae: 19.5395\n",
      "Epoch 153/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.9900 - mae: 19.6314 - val_loss: 564.7121 - val_mae: 19.4670\n",
      "Epoch 154/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.1154 - mae: 19.6030 - val_loss: 562.8297 - val_mae: 19.5345\n",
      "Epoch 155/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 574.1293 - mae: 19.6177 - val_loss: 582.8840 - val_mae: 19.7124\n",
      "Epoch 156/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.7364 - mae: 19.6098 - val_loss: 563.8213 - val_mae: 19.5227\n",
      "Epoch 157/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 574.2177 - mae: 19.6236 - val_loss: 567.1995 - val_mae: 19.7077\n",
      "Epoch 158/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.3417 - mae: 19.6184 - val_loss: 574.0074 - val_mae: 19.5080\n",
      "Epoch 159/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 572.7101 - mae: 19.6052 - val_loss: 563.1580 - val_mae: 19.5021\n",
      "Epoch 160/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.9997 - mae: 19.6046 - val_loss: 564.3798 - val_mae: 19.6021\n",
      "Epoch 161/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.0131 - mae: 19.6034 - val_loss: 563.7216 - val_mae: 19.5647\n",
      "Epoch 162/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 573.1487 - mae: 19.6034 - val_loss: 564.1039 - val_mae: 19.5906\n",
      "Epoch 163/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.3341 - mae: 19.5914 - val_loss: 563.5583 - val_mae: 19.5760\n",
      "Epoch 164/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 572.6323 - mae: 19.6052 - val_loss: 562.9117 - val_mae: 19.5724\n",
      "Epoch 165/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 573.5561 - mae: 19.6225 - val_loss: 564.0476 - val_mae: 19.6180\n",
      "Epoch 166/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.5370 - mae: 19.6053 - val_loss: 563.3004 - val_mae: 19.5850\n",
      "Epoch 167/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 572.2090 - mae: 19.6017 - val_loss: 565.4620 - val_mae: 19.5022\n",
      "Epoch 168/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 573.2977 - mae: 19.6194 - val_loss: 563.5963 - val_mae: 19.5713\n",
      "Epoch 169/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.7818 - mae: 19.6076 - val_loss: 562.7471 - val_mae: 19.5472\n",
      "Epoch 170/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.5930 - mae: 19.6065 - val_loss: 563.9561 - val_mae: 19.5212\n",
      "Epoch 171/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 572.3821 - mae: 19.5950 - val_loss: 564.5842 - val_mae: 19.6323\n",
      "Epoch 172/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 573.0333 - mae: 19.6122 - val_loss: 563.0798 - val_mae: 19.5662\n",
      "Epoch 173/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.0500 - mae: 19.5933 - val_loss: 564.9413 - val_mae: 19.5640\n",
      "Epoch 174/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 573.5115 - mae: 19.6157 - val_loss: 563.1640 - val_mae: 19.5732\n",
      "Epoch 175/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.3737 - mae: 19.6112 - val_loss: 563.6528 - val_mae: 19.4952\n",
      "Epoch 176/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.9066 - mae: 19.6090 - val_loss: 566.3425 - val_mae: 19.4600\n",
      "Epoch 177/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 573.2095 - mae: 19.6011 - val_loss: 564.0436 - val_mae: 19.4851\n",
      "Epoch 178/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.3673 - mae: 19.6031 - val_loss: 562.5640 - val_mae: 19.5384\n",
      "Epoch 179/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 573.3940 - mae: 19.6092 - val_loss: 592.3863 - val_mae: 20.2692\n",
      "Epoch 180/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 573.9755 - mae: 19.6366 - val_loss: 562.8798 - val_mae: 19.5696\n",
      "Epoch 181/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 573.3381 - mae: 19.6185 - val_loss: 564.5764 - val_mae: 19.6255\n",
      "Epoch 182/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 572.0444 - mae: 19.5941 - val_loss: 563.4227 - val_mae: 19.5122\n",
      "Epoch 183/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.9331 - mae: 19.6023 - val_loss: 566.0111 - val_mae: 19.6675\n",
      "Epoch 184/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.9341 - mae: 19.6038 - val_loss: 563.7540 - val_mae: 19.4707\n",
      "Epoch 185/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.8915 - mae: 19.6209 - val_loss: 562.6917 - val_mae: 19.5486\n",
      "Epoch 186/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.9104 - mae: 19.6157 - val_loss: 571.8536 - val_mae: 19.8278\n",
      "Epoch 187/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.7406 - mae: 19.6101 - val_loss: 565.6298 - val_mae: 19.6425\n",
      "Epoch 188/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.9459 - mae: 19.6164 - val_loss: 570.2035 - val_mae: 19.4702\n",
      "Epoch 189/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 573.0852 - mae: 19.6169 - val_loss: 562.9456 - val_mae: 19.5261\n",
      "Epoch 190/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 571.9658 - mae: 19.5967 - val_loss: 563.8953 - val_mae: 19.6020\n",
      "Epoch 191/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.6448 - mae: 19.6044 - val_loss: 565.3177 - val_mae: 19.4822\n",
      "Epoch 192/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.0457 - mae: 19.6010 - val_loss: 566.5239 - val_mae: 19.6948\n",
      "Epoch 193/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 572.3701 - mae: 19.6118 - val_loss: 563.9175 - val_mae: 19.4692\n",
      "Epoch 194/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.3009 - mae: 19.5795 - val_loss: 565.7920 - val_mae: 19.6788\n",
      "Epoch 195/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.9905 - mae: 19.6201 - val_loss: 570.0923 - val_mae: 19.4798\n",
      "Epoch 196/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.2150 - mae: 19.5961 - val_loss: 563.6376 - val_mae: 19.4837\n",
      "Epoch 197/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.9482 - mae: 19.5905 - val_loss: 565.5599 - val_mae: 19.6681\n",
      "Epoch 198/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.1025 - mae: 19.5929 - val_loss: 564.3417 - val_mae: 19.6293\n",
      "Epoch 199/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.2145 - mae: 19.6105 - val_loss: 562.1752 - val_mae: 19.5419\n",
      "Epoch 200/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.0405 - mae: 19.6009 - val_loss: 569.4053 - val_mae: 19.4924\n",
      "Epoch 201/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 573.0122 - mae: 19.6007 - val_loss: 570.1526 - val_mae: 19.7881\n",
      "Epoch 202/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.7362 - mae: 19.6078 - val_loss: 562.9913 - val_mae: 19.5361\n",
      "Epoch 203/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.8015 - mae: 19.6066 - val_loss: 563.4194 - val_mae: 19.5903\n",
      "Epoch 204/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.5760 - mae: 19.6176 - val_loss: 571.3075 - val_mae: 19.8205\n",
      "Epoch 205/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.5193 - mae: 19.6123 - val_loss: 566.3232 - val_mae: 19.4753\n",
      "Epoch 206/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 572.6132 - mae: 19.6099 - val_loss: 562.8414 - val_mae: 19.5293\n",
      "Epoch 207/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.0061 - mae: 19.5955 - val_loss: 566.4528 - val_mae: 19.7005\n",
      "Epoch 208/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.4590 - mae: 19.5995 - val_loss: 563.3872 - val_mae: 19.5995\n",
      "Epoch 209/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.5557 - mae: 19.5858 - val_loss: 563.5621 - val_mae: 19.5589\n",
      "Epoch 210/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.5204 - mae: 19.6067 - val_loss: 562.5762 - val_mae: 19.5525\n",
      "Epoch 211/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.1426 - mae: 19.5916 - val_loss: 562.4291 - val_mae: 19.5661\n",
      "Epoch 212/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.7884 - mae: 19.6212 - val_loss: 562.1309 - val_mae: 19.5244\n",
      "Epoch 213/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.1221 - mae: 19.6027 - val_loss: 563.6646 - val_mae: 19.4824\n",
      "Epoch 214/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.6078 - mae: 19.6058 - val_loss: 564.1581 - val_mae: 19.6306\n",
      "Epoch 215/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.0428 - mae: 19.5856 - val_loss: 564.4690 - val_mae: 19.6395\n",
      "Epoch 216/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.1358 - mae: 19.6021 - val_loss: 562.6548 - val_mae: 19.5054\n",
      "Epoch 217/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 571.6929 - mae: 19.5798 - val_loss: 562.5994 - val_mae: 19.5572\n",
      "Epoch 218/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.4039 - mae: 19.6015 - val_loss: 572.1364 - val_mae: 19.8106\n",
      "Epoch 219/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 572.0541 - mae: 19.6071 - val_loss: 562.8721 - val_mae: 19.5632\n",
      "Epoch 220/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.6255 - mae: 19.6033 - val_loss: 563.5251 - val_mae: 19.4966\n",
      "Epoch 221/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.0752 - mae: 19.5970 - val_loss: 566.3698 - val_mae: 19.4591\n",
      "Epoch 222/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.1574 - mae: 19.6023 - val_loss: 562.6262 - val_mae: 19.4991\n",
      "Epoch 223/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.8586 - mae: 19.6008 - val_loss: 562.3193 - val_mae: 19.5422\n",
      "Epoch 224/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.8577 - mae: 19.5928 - val_loss: 566.6150 - val_mae: 19.4674\n",
      "Epoch 225/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.3192 - mae: 19.5672 - val_loss: 598.9257 - val_mae: 20.3855\n",
      "Epoch 226/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 572.0228 - mae: 19.6037 - val_loss: 564.9980 - val_mae: 19.4619\n",
      "Epoch 227/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.1365 - mae: 19.5993 - val_loss: 563.3041 - val_mae: 19.5886\n",
      "Epoch 228/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.3411 - mae: 19.5872 - val_loss: 563.5633 - val_mae: 19.4687\n",
      "Epoch 229/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 571.6508 - mae: 19.5921 - val_loss: 563.2045 - val_mae: 19.5931\n",
      "Epoch 230/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.9949 - mae: 19.5877 - val_loss: 563.3686 - val_mae: 19.5964\n",
      "Epoch 231/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.6306 - mae: 19.5985 - val_loss: 561.9794 - val_mae: 19.5373\n",
      "Epoch 232/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.0319 - mae: 19.5954 - val_loss: 566.6404 - val_mae: 19.4728\n",
      "Epoch 233/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 572.0174 - mae: 19.5995 - val_loss: 563.8248 - val_mae: 19.6148\n",
      "Epoch 234/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.2454 - mae: 19.6131 - val_loss: 564.7659 - val_mae: 19.6229\n",
      "Epoch 235/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.2062 - mae: 19.6031 - val_loss: 562.7615 - val_mae: 19.5172\n",
      "Epoch 236/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.6466 - mae: 19.5858 - val_loss: 562.1905 - val_mae: 19.5004\n",
      "Epoch 237/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.0684 - mae: 19.5871 - val_loss: 563.2861 - val_mae: 19.5760\n",
      "Epoch 238/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 572.0005 - mae: 19.6075 - val_loss: 565.3953 - val_mae: 19.4518\n",
      "Epoch 239/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.6595 - mae: 19.5955 - val_loss: 563.7532 - val_mae: 19.6057\n",
      "Epoch 240/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 571.7405 - mae: 19.5844 - val_loss: 567.8901 - val_mae: 19.7237\n",
      "Epoch 241/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.6967 - mae: 19.5844 - val_loss: 562.0609 - val_mae: 19.5165\n",
      "Epoch 242/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.9846 - mae: 19.6015 - val_loss: 562.0718 - val_mae: 19.4950\n",
      "Epoch 243/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 571.1201 - mae: 19.5890 - val_loss: 565.4293 - val_mae: 19.4591\n",
      "Epoch 244/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.8997 - mae: 19.5868 - val_loss: 569.5303 - val_mae: 19.7631\n",
      "Epoch 245/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 572.5552 - mae: 19.5995 - val_loss: 566.1314 - val_mae: 19.6722\n",
      "Epoch 246/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 571.7926 - mae: 19.5850 - val_loss: 566.7468 - val_mae: 19.4705\n",
      "Epoch 247/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.2968 - mae: 19.5935 - val_loss: 563.7209 - val_mae: 19.6248\n",
      "Epoch 248/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.6208 - mae: 19.5964 - val_loss: 564.5530 - val_mae: 19.4861\n",
      "Epoch 249/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.7299 - mae: 19.5904 - val_loss: 562.7565 - val_mae: 19.4801\n",
      "Epoch 250/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.0752 - mae: 19.5798 - val_loss: 565.3632 - val_mae: 19.4736\n",
      "Epoch 251/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 571.5664 - mae: 19.5910 - val_loss: 563.2615 - val_mae: 19.5495\n",
      "Epoch 252/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 572.1649 - mae: 19.6084 - val_loss: 563.0961 - val_mae: 19.5970\n",
      "Epoch 253/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 570.8011 - mae: 19.5839 - val_loss: 562.1061 - val_mae: 19.5146\n",
      "Epoch 254/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 571.2888 - mae: 19.5847 - val_loss: 567.6686 - val_mae: 19.4854\n",
      "Epoch 255/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.6862 - mae: 19.5836 - val_loss: 565.4068 - val_mae: 19.4710\n",
      "Epoch 256/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.7922 - mae: 19.6067 - val_loss: 572.9026 - val_mae: 19.4975\n",
      "Epoch 257/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.4117 - mae: 19.5876 - val_loss: 564.5513 - val_mae: 19.4583\n",
      "Epoch 258/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.3477 - mae: 19.5888 - val_loss: 563.0812 - val_mae: 19.4995\n",
      "Epoch 259/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.1992 - mae: 19.5834 - val_loss: 561.9883 - val_mae: 19.5305\n",
      "Epoch 260/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.8580 - mae: 19.5977 - val_loss: 569.9099 - val_mae: 19.7912\n",
      "Epoch 261/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 571.6671 - mae: 19.5880 - val_loss: 564.1703 - val_mae: 19.6273\n",
      "Epoch 262/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.8104 - mae: 19.5845 - val_loss: 562.3839 - val_mae: 19.5674\n",
      "Epoch 263/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 572.1210 - mae: 19.6121 - val_loss: 562.2586 - val_mae: 19.5260\n",
      "Epoch 264/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.2612 - mae: 19.5865 - val_loss: 565.8041 - val_mae: 19.6743\n",
      "Epoch 265/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.4384 - mae: 19.5999 - val_loss: 564.5070 - val_mae: 19.4547\n",
      "Epoch 266/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 571.7905 - mae: 19.5917 - val_loss: 568.3403 - val_mae: 19.4641\n",
      "Epoch 267/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.5569 - mae: 19.5816 - val_loss: 564.6614 - val_mae: 19.4632\n",
      "Epoch 268/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 571.1404 - mae: 19.5785 - val_loss: 562.9715 - val_mae: 19.4796\n",
      "Epoch 269/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 570.9933 - mae: 19.5899 - val_loss: 563.4664 - val_mae: 19.6009\n",
      "Epoch 270/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.5220 - mae: 19.5860 - val_loss: 564.4915 - val_mae: 19.6314\n",
      "Epoch 271/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 571.2625 - mae: 19.5905 - val_loss: 565.8917 - val_mae: 19.6879\n",
      "Epoch 272/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.0341 - mae: 19.5964 - val_loss: 562.3447 - val_mae: 19.4820\n",
      "Epoch 273/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.1034 - mae: 19.5879 - val_loss: 562.4644 - val_mae: 19.5734\n",
      "Epoch 274/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 571.9470 - mae: 19.5984 - val_loss: 563.3749 - val_mae: 19.4753\n",
      "Epoch 275/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.2137 - mae: 19.5858 - val_loss: 568.1392 - val_mae: 19.7414\n",
      "Epoch 276/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.1187 - mae: 19.5869 - val_loss: 562.4070 - val_mae: 19.4677\n",
      "Epoch 277/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.5753 - mae: 19.5958 - val_loss: 563.5333 - val_mae: 19.6227\n",
      "Epoch 278/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 572.4713 - mae: 19.5979 - val_loss: 561.9775 - val_mae: 19.4954\n",
      "Epoch 279/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.0455 - mae: 19.5691 - val_loss: 561.6005 - val_mae: 19.5467\n",
      "Epoch 280/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.4731 - mae: 19.5988 - val_loss: 562.3170 - val_mae: 19.4911\n",
      "Epoch 281/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.9637 - mae: 19.5821 - val_loss: 563.7098 - val_mae: 19.5236\n",
      "Epoch 282/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.7213 - mae: 19.6005 - val_loss: 567.4402 - val_mae: 19.7251\n",
      "Epoch 283/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.2131 - mae: 19.6027 - val_loss: 561.9902 - val_mae: 19.4789\n",
      "Epoch 284/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.4114 - mae: 19.5861 - val_loss: 563.0853 - val_mae: 19.5967\n",
      "Epoch 285/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.2270 - mae: 19.5860 - val_loss: 561.7554 - val_mae: 19.4793\n",
      "Epoch 286/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.6614 - mae: 19.5798 - val_loss: 565.6472 - val_mae: 19.6813\n",
      "Epoch 287/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.3613 - mae: 19.5828 - val_loss: 562.0626 - val_mae: 19.5656\n",
      "Epoch 288/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.0101 - mae: 19.5907 - val_loss: 564.6941 - val_mae: 19.4539\n",
      "Epoch 289/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.7562 - mae: 19.5956 - val_loss: 563.8288 - val_mae: 19.4737\n",
      "Epoch 290/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.5520 - mae: 19.5930 - val_loss: 562.4294 - val_mae: 19.5786\n",
      "Epoch 291/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.5850 - mae: 19.5966 - val_loss: 562.0356 - val_mae: 19.4784\n",
      "Epoch 292/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.3298 - mae: 19.5904 - val_loss: 562.5433 - val_mae: 19.5112\n",
      "Epoch 293/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.7620 - mae: 19.5980 - val_loss: 567.6655 - val_mae: 19.7286\n",
      "Epoch 294/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.2855 - mae: 19.5970 - val_loss: 561.6582 - val_mae: 19.5164\n",
      "Epoch 295/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.1422 - mae: 19.5800 - val_loss: 565.9856 - val_mae: 19.6839\n",
      "Epoch 296/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.0361 - mae: 19.5904 - val_loss: 563.1659 - val_mae: 19.5973\n",
      "Epoch 297/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.0016 - mae: 19.5853 - val_loss: 567.3954 - val_mae: 19.7326\n",
      "Epoch 298/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 570.7935 - mae: 19.5840 - val_loss: 567.9031 - val_mae: 19.7480\n",
      "Epoch 299/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.7449 - mae: 19.5701 - val_loss: 561.5663 - val_mae: 19.5176\n",
      "Epoch 300/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.7708 - mae: 19.5966 - val_loss: 562.3049 - val_mae: 19.4779\n",
      "Epoch 301/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 570.7733 - mae: 19.5896 - val_loss: 565.7405 - val_mae: 19.5378\n",
      "Epoch 302/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.6107 - mae: 19.5674 - val_loss: 563.2292 - val_mae: 19.5854\n",
      "Epoch 303/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 570.9092 - mae: 19.5845 - val_loss: 563.5351 - val_mae: 19.5888\n",
      "Epoch 304/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.6302 - mae: 19.5880 - val_loss: 561.5582 - val_mae: 19.4978\n",
      "Epoch 305/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.0179 - mae: 19.5851 - val_loss: 561.9440 - val_mae: 19.4817\n",
      "Epoch 306/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.5058 - mae: 19.5825 - val_loss: 562.6995 - val_mae: 19.5293\n",
      "Epoch 307/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.0640 - mae: 19.5840 - val_loss: 564.9569 - val_mae: 19.6511\n",
      "Epoch 308/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 571.1796 - mae: 19.5939 - val_loss: 566.3661 - val_mae: 19.6990\n",
      "Epoch 309/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.5001 - mae: 19.5774 - val_loss: 562.1761 - val_mae: 19.4884\n",
      "Epoch 310/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 571.1144 - mae: 19.5890 - val_loss: 561.3168 - val_mae: 19.4753\n",
      "Epoch 311/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.8105 - mae: 19.5745 - val_loss: 568.0060 - val_mae: 19.4745\n",
      "Epoch 312/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.9879 - mae: 19.5879 - val_loss: 566.6648 - val_mae: 19.7133\n",
      "Epoch 313/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.6207 - mae: 19.5900 - val_loss: 563.7366 - val_mae: 19.4599\n",
      "Epoch 314/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 570.9128 - mae: 19.5864 - val_loss: 563.4749 - val_mae: 19.4688\n",
      "Epoch 315/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.6229 - mae: 19.5840 - val_loss: 561.4851 - val_mae: 19.5409\n",
      "Epoch 316/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.9531 - mae: 19.5962 - val_loss: 561.3107 - val_mae: 19.5361\n",
      "Epoch 317/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.2214 - mae: 19.5616 - val_loss: 561.5889 - val_mae: 19.5574\n",
      "Epoch 318/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 570.5400 - mae: 19.5711 - val_loss: 565.8157 - val_mae: 19.6817\n",
      "Epoch 319/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.0050 - mae: 19.5690 - val_loss: 565.2105 - val_mae: 19.6334\n",
      "Epoch 320/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.6245 - mae: 19.5886 - val_loss: 563.8173 - val_mae: 19.6201\n",
      "Epoch 321/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 570.8934 - mae: 19.5888 - val_loss: 562.1201 - val_mae: 19.5448\n",
      "Epoch 322/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 570.7064 - mae: 19.5838 - val_loss: 562.2845 - val_mae: 19.4814\n",
      "Epoch 323/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 570.8618 - mae: 19.5953 - val_loss: 561.3478 - val_mae: 19.4922\n",
      "Epoch 324/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.3821 - mae: 19.5786 - val_loss: 561.4332 - val_mae: 19.4757\n",
      "Epoch 325/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.3956 - mae: 19.5784 - val_loss: 561.1567 - val_mae: 19.4932\n",
      "Epoch 326/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.7888 - mae: 19.5687 - val_loss: 564.2573 - val_mae: 19.6506\n",
      "Epoch 327/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.5819 - mae: 19.5922 - val_loss: 562.3454 - val_mae: 19.4888\n",
      "Epoch 328/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.1047 - mae: 19.5651 - val_loss: 563.4324 - val_mae: 19.5802\n",
      "Epoch 329/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.8175 - mae: 19.5900 - val_loss: 561.2820 - val_mae: 19.4670\n",
      "Epoch 330/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.8306 - mae: 19.5848 - val_loss: 563.9269 - val_mae: 19.6149\n",
      "Epoch 331/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.3666 - mae: 19.5783 - val_loss: 563.1615 - val_mae: 19.4525\n",
      "Epoch 332/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.2262 - mae: 19.5724 - val_loss: 561.4600 - val_mae: 19.5297\n",
      "Epoch 333/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.2683 - mae: 19.5799 - val_loss: 561.8997 - val_mae: 19.4791\n",
      "Epoch 334/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.3998 - mae: 19.5699 - val_loss: 565.1238 - val_mae: 19.6540\n",
      "Epoch 335/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.9666 - mae: 19.5842 - val_loss: 563.9230 - val_mae: 19.5092\n",
      "Epoch 336/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.0561 - mae: 19.5728 - val_loss: 561.6854 - val_mae: 19.5116\n",
      "Epoch 337/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.4526 - mae: 19.5812 - val_loss: 561.4584 - val_mae: 19.4862\n",
      "Epoch 338/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.7216 - mae: 19.5731 - val_loss: 563.0400 - val_mae: 19.4657\n",
      "Epoch 339/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.1318 - mae: 19.5705 - val_loss: 562.0648 - val_mae: 19.5795\n",
      "Epoch 340/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.4161 - mae: 19.5877 - val_loss: 561.7830 - val_mae: 19.5384\n",
      "Epoch 341/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.1392 - mae: 19.5765 - val_loss: 561.8733 - val_mae: 19.4693\n",
      "Epoch 342/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.9260 - mae: 19.5628 - val_loss: 563.2266 - val_mae: 19.6107\n",
      "Epoch 343/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.6869 - mae: 19.5721 - val_loss: 561.9916 - val_mae: 19.4888\n",
      "Epoch 344/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.6027 - mae: 19.5645 - val_loss: 560.7916 - val_mae: 19.5047\n",
      "Epoch 345/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.7094 - mae: 19.5554 - val_loss: 562.1714 - val_mae: 19.5070\n",
      "Epoch 346/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.1113 - mae: 19.5699 - val_loss: 561.0850 - val_mae: 19.4828\n",
      "Epoch 347/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.2527 - mae: 19.5506 - val_loss: 561.8608 - val_mae: 19.5323\n",
      "Epoch 348/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.3777 - mae: 19.5829 - val_loss: 562.0693 - val_mae: 19.5159\n",
      "Epoch 349/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.1752 - mae: 19.5721 - val_loss: 561.9412 - val_mae: 19.4949\n",
      "Epoch 350/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.8580 - mae: 19.5814 - val_loss: 564.9269 - val_mae: 19.6397\n",
      "Epoch 351/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.8253 - mae: 19.5631 - val_loss: 564.6007 - val_mae: 19.4736\n",
      "Epoch 352/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.2068 - mae: 19.5679 - val_loss: 564.6163 - val_mae: 19.6338\n",
      "Epoch 353/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.8301 - mae: 19.5643 - val_loss: 563.4335 - val_mae: 19.5959\n",
      "Epoch 354/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.0053 - mae: 19.5737 - val_loss: 562.0731 - val_mae: 19.4756\n",
      "Epoch 355/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.9264 - mae: 19.5694 - val_loss: 561.7919 - val_mae: 19.4949\n",
      "Epoch 356/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.6479 - mae: 19.5596 - val_loss: 563.6790 - val_mae: 19.4649\n",
      "Epoch 357/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.6155 - mae: 19.5551 - val_loss: 561.8163 - val_mae: 19.5141\n",
      "Epoch 358/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.0794 - mae: 19.5832 - val_loss: 566.6899 - val_mae: 19.7064\n",
      "Epoch 359/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.4061 - mae: 19.5521 - val_loss: 564.4107 - val_mae: 19.6300\n",
      "Epoch 360/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.6664 - mae: 19.5647 - val_loss: 561.3823 - val_mae: 19.5087\n",
      "Epoch 361/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.6323 - mae: 19.5655 - val_loss: 562.4139 - val_mae: 19.5109\n",
      "Epoch 362/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 570.1531 - mae: 19.5703 - val_loss: 563.5482 - val_mae: 19.6193\n",
      "Epoch 363/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.4445 - mae: 19.5776 - val_loss: 561.1711 - val_mae: 19.4930\n",
      "Epoch 364/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.5876 - mae: 19.5726 - val_loss: 561.8613 - val_mae: 19.4893\n",
      "Epoch 365/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.1405 - mae: 19.5712 - val_loss: 562.3187 - val_mae: 19.5056\n",
      "Epoch 366/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.0844 - mae: 19.5713 - val_loss: 561.6925 - val_mae: 19.4786\n",
      "Epoch 367/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.9913 - mae: 19.5980 - val_loss: 565.3767 - val_mae: 19.4926\n",
      "Epoch 368/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.3200 - mae: 19.5670 - val_loss: 562.6423 - val_mae: 19.5440\n",
      "Epoch 369/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.4540 - mae: 19.5694 - val_loss: 566.7117 - val_mae: 19.4429\n",
      "Epoch 370/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.5181 - mae: 19.5765 - val_loss: 565.3792 - val_mae: 19.6391\n",
      "Epoch 371/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.3442 - mae: 19.5756 - val_loss: 573.5162 - val_mae: 19.8600\n",
      "Epoch 372/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.6104 - mae: 19.5668 - val_loss: 563.5016 - val_mae: 19.5023\n",
      "Epoch 373/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 569.3340 - mae: 19.5611 - val_loss: 561.3534 - val_mae: 19.5351\n",
      "Epoch 374/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.8983 - mae: 19.5846 - val_loss: 561.7299 - val_mae: 19.4997\n",
      "Epoch 375/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.4219 - mae: 19.5569 - val_loss: 564.5955 - val_mae: 19.5979\n",
      "Epoch 376/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 570.2355 - mae: 19.5813 - val_loss: 561.5229 - val_mae: 19.5083\n",
      "Epoch 377/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.7235 - mae: 19.5707 - val_loss: 561.3798 - val_mae: 19.4984\n",
      "Epoch 378/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.1320 - mae: 19.5559 - val_loss: 566.3780 - val_mae: 19.7110\n",
      "Epoch 379/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.1353 - mae: 19.5667 - val_loss: 568.1208 - val_mae: 19.7226\n",
      "Epoch 380/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.0477 - mae: 19.5767 - val_loss: 565.1490 - val_mae: 19.6477\n",
      "Epoch 381/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.5065 - mae: 19.5703 - val_loss: 561.3688 - val_mae: 19.4689\n",
      "Epoch 382/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.3821 - mae: 19.5673 - val_loss: 561.6179 - val_mae: 19.5591\n",
      "Epoch 383/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.3514 - mae: 19.5713 - val_loss: 562.6563 - val_mae: 19.4497\n",
      "Epoch 384/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.6708 - mae: 19.5646 - val_loss: 561.5856 - val_mae: 19.5230\n",
      "Epoch 385/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5572 - mae: 19.5519 - val_loss: 563.5895 - val_mae: 19.5660\n",
      "Epoch 386/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.0425 - mae: 19.5587 - val_loss: 562.1741 - val_mae: 19.5903\n",
      "Epoch 387/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.4013 - mae: 19.5738 - val_loss: 562.4955 - val_mae: 19.4730\n",
      "Epoch 388/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.3685 - mae: 19.5627 - val_loss: 570.0428 - val_mae: 19.4490\n",
      "Epoch 389/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.5269 - mae: 19.5745 - val_loss: 561.2003 - val_mae: 19.5293\n",
      "Epoch 390/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.7158 - mae: 19.5710 - val_loss: 561.4954 - val_mae: 19.5298\n",
      "Epoch 391/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 569.2290 - mae: 19.5582 - val_loss: 561.8096 - val_mae: 19.5066\n",
      "Epoch 392/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.2185 - mae: 19.5623 - val_loss: 562.5827 - val_mae: 19.5890\n",
      "Epoch 393/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.8232 - mae: 19.5586 - val_loss: 564.5792 - val_mae: 19.5388\n",
      "Epoch 394/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.0073 - mae: 19.5670 - val_loss: 564.4371 - val_mae: 19.5553\n",
      "Epoch 395/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.3555 - mae: 19.5667 - val_loss: 564.2945 - val_mae: 19.4819\n",
      "Epoch 396/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.4457 - mae: 19.5622 - val_loss: 562.3182 - val_mae: 19.5689\n",
      "Epoch 397/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.8394 - mae: 19.5766 - val_loss: 562.4894 - val_mae: 19.5517\n",
      "Epoch 398/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.5651 - mae: 19.5642 - val_loss: 563.5914 - val_mae: 19.5789\n",
      "Epoch 399/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.4213 - mae: 19.5723 - val_loss: 562.5405 - val_mae: 19.4664\n",
      "Epoch 400/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 569.4088 - mae: 19.5616 - val_loss: 563.4948 - val_mae: 19.5041\n",
      "Epoch 401/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.3121 - mae: 19.5537 - val_loss: 562.4190 - val_mae: 19.4949\n",
      "Epoch 402/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.5276 - mae: 19.5703 - val_loss: 562.4985 - val_mae: 19.5089\n",
      "Epoch 403/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9733 - mae: 19.5496 - val_loss: 560.7559 - val_mae: 19.4921\n",
      "Epoch 404/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 568.7723 - mae: 19.5459 - val_loss: 562.2241 - val_mae: 19.5651\n",
      "Epoch 405/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 568.9297 - mae: 19.5584 - val_loss: 565.2416 - val_mae: 19.6750\n",
      "Epoch 406/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9574 - mae: 19.5612 - val_loss: 562.8896 - val_mae: 19.6059\n",
      "Epoch 407/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 569.0529 - mae: 19.5634 - val_loss: 562.4333 - val_mae: 19.5519\n",
      "Epoch 408/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.2400 - mae: 19.5631 - val_loss: 563.7304 - val_mae: 19.6312\n",
      "Epoch 409/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.6765 - mae: 19.5475 - val_loss: 563.6467 - val_mae: 19.5072\n",
      "Epoch 410/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.0499 - mae: 19.5524 - val_loss: 561.3618 - val_mae: 19.5306\n",
      "Epoch 411/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 568.9498 - mae: 19.5574 - val_loss: 561.4435 - val_mae: 19.5269\n",
      "Epoch 412/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1940 - mae: 19.5407 - val_loss: 562.1298 - val_mae: 19.4978\n",
      "Epoch 413/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4347 - mae: 19.5496 - val_loss: 562.6998 - val_mae: 19.4690\n",
      "Epoch 414/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.2796 - mae: 19.5646 - val_loss: 562.8035 - val_mae: 19.5528\n",
      "Epoch 415/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.2606 - mae: 19.5641 - val_loss: 561.8383 - val_mae: 19.4963\n",
      "Epoch 416/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.0865 - mae: 19.5600 - val_loss: 561.7122 - val_mae: 19.5129\n",
      "Epoch 417/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8254 - mae: 19.5510 - val_loss: 560.9202 - val_mae: 19.4881\n",
      "Epoch 418/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9612 - mae: 19.5640 - val_loss: 561.9963 - val_mae: 19.4850\n",
      "Epoch 419/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7564 - mae: 19.5609 - val_loss: 563.9963 - val_mae: 19.4318\n",
      "Epoch 420/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7307 - mae: 19.5543 - val_loss: 560.5928 - val_mae: 19.4785\n",
      "Epoch 421/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4670 - mae: 19.5593 - val_loss: 563.4131 - val_mae: 19.5129\n",
      "Epoch 422/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.2277 - mae: 19.5592 - val_loss: 562.9501 - val_mae: 19.5621\n",
      "Epoch 423/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5915 - mae: 19.5554 - val_loss: 560.8873 - val_mae: 19.4770\n",
      "Epoch 424/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7919 - mae: 19.5532 - val_loss: 562.6353 - val_mae: 19.5728\n",
      "Epoch 425/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5635 - mae: 19.5528 - val_loss: 561.2048 - val_mae: 19.4896\n",
      "Epoch 426/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.6925 - mae: 19.5558 - val_loss: 560.5845 - val_mae: 19.4966\n",
      "Epoch 427/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.6331 - mae: 19.5579 - val_loss: 562.5643 - val_mae: 19.5891\n",
      "Epoch 428/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5056 - mae: 19.5482 - val_loss: 564.4265 - val_mae: 19.5551\n",
      "Epoch 429/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8080 - mae: 19.5486 - val_loss: 561.3834 - val_mae: 19.5307\n",
      "Epoch 430/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8329 - mae: 19.5500 - val_loss: 561.7022 - val_mae: 19.5564\n",
      "Epoch 431/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5304 - mae: 19.5512 - val_loss: 560.0721 - val_mae: 19.4754\n",
      "Epoch 432/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.0089 - mae: 19.5559 - val_loss: 561.3715 - val_mae: 19.5018\n",
      "Epoch 433/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4448 - mae: 19.5612 - val_loss: 563.6958 - val_mae: 19.5109\n",
      "Epoch 434/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5818 - mae: 19.5493 - val_loss: 560.9283 - val_mae: 19.5306\n",
      "Epoch 435/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.6548 - mae: 19.5573 - val_loss: 561.4133 - val_mae: 19.4872\n",
      "Epoch 436/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 568.4685 - mae: 19.5479 - val_loss: 562.7753 - val_mae: 19.4687\n",
      "Epoch 437/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9373 - mae: 19.5649 - val_loss: 562.3176 - val_mae: 19.5449\n",
      "Epoch 438/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3853 - mae: 19.5540 - val_loss: 561.9622 - val_mae: 19.5528\n",
      "Epoch 439/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7558 - mae: 19.5643 - val_loss: 561.9119 - val_mae: 19.4707\n",
      "Epoch 440/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5815 - mae: 19.5469 - val_loss: 566.4111 - val_mae: 19.6774\n",
      "Epoch 441/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4907 - mae: 19.5579 - val_loss: 564.2488 - val_mae: 19.6372\n",
      "Epoch 442/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4967 - mae: 19.5631 - val_loss: 564.1367 - val_mae: 19.5353\n",
      "Epoch 443/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4489 - mae: 19.5485 - val_loss: 566.3327 - val_mae: 19.6798\n",
      "Epoch 444/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.6537 - mae: 19.5641 - val_loss: 563.7626 - val_mae: 19.5273\n",
      "Epoch 445/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3794 - mae: 19.5516 - val_loss: 562.0370 - val_mae: 19.4864\n",
      "Epoch 446/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4820 - mae: 19.5492 - val_loss: 562.7592 - val_mae: 19.5796\n",
      "Epoch 447/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3140 - mae: 19.5419 - val_loss: 561.0170 - val_mae: 19.4930\n",
      "Epoch 448/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2309 - mae: 19.5554 - val_loss: 564.2211 - val_mae: 19.5124\n",
      "Epoch 449/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2676 - mae: 19.5503 - val_loss: 561.7104 - val_mae: 19.5449\n",
      "Epoch 450/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3220 - mae: 19.5614 - val_loss: 561.0807 - val_mae: 19.4739\n",
      "Epoch 451/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2451 - mae: 19.5466 - val_loss: 562.7866 - val_mae: 19.6062\n",
      "Epoch 452/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.6888 - mae: 19.5532 - val_loss: 564.8992 - val_mae: 19.6499\n",
      "Epoch 453/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5500 - mae: 19.5583 - val_loss: 562.0881 - val_mae: 19.4722\n",
      "Epoch 454/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2509 - mae: 19.5511 - val_loss: 562.2980 - val_mae: 19.5357\n",
      "Epoch 455/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0386 - mae: 19.5435 - val_loss: 561.0438 - val_mae: 19.4710\n",
      "Epoch 456/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 567.9363 - mae: 19.5507 - val_loss: 561.7147 - val_mae: 19.4439\n",
      "Epoch 457/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2767 - mae: 19.5435 - val_loss: 561.2390 - val_mae: 19.4867\n",
      "Epoch 458/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 567.7172 - mae: 19.5451 - val_loss: 561.9181 - val_mae: 19.4972\n",
      "Epoch 459/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3187 - mae: 19.5613 - val_loss: 561.9313 - val_mae: 19.5973\n",
      "Epoch 460/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9576 - mae: 19.5614 - val_loss: 563.4386 - val_mae: 19.5026\n",
      "Epoch 461/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.1331 - mae: 19.5531 - val_loss: 562.2433 - val_mae: 19.5638\n",
      "Epoch 462/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 567.9536 - mae: 19.5449 - val_loss: 567.9547 - val_mae: 19.7554\n",
      "Epoch 463/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8094 - mae: 19.5661 - val_loss: 562.8069 - val_mae: 19.5741\n",
      "Epoch 464/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7324 - mae: 19.5587 - val_loss: 562.0391 - val_mae: 19.4906\n",
      "Epoch 465/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 568.8506 - mae: 19.5605 - val_loss: 562.8608 - val_mae: 19.5093\n",
      "Epoch 466/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5704 - mae: 19.5520 - val_loss: 562.2640 - val_mae: 19.4945\n",
      "Epoch 467/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5821 - mae: 19.5565 - val_loss: 561.9127 - val_mae: 19.5763\n",
      "Epoch 468/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1590 - mae: 19.5535 - val_loss: 561.6484 - val_mae: 19.4528\n",
      "Epoch 469/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1298 - mae: 19.5533 - val_loss: 561.2275 - val_mae: 19.5002\n",
      "Epoch 470/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0855 - mae: 19.5390 - val_loss: 562.9600 - val_mae: 19.5283\n",
      "Epoch 471/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0873 - mae: 19.5447 - val_loss: 561.4800 - val_mae: 19.5427\n",
      "Epoch 472/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 567.9037 - mae: 19.5476 - val_loss: 564.1836 - val_mae: 19.4665\n",
      "Epoch 473/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3143 - mae: 19.5488 - val_loss: 561.9764 - val_mae: 19.5371\n",
      "Epoch 474/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1960 - mae: 19.5557 - val_loss: 562.0974 - val_mae: 19.5002\n",
      "Epoch 475/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1981 - mae: 19.5539 - val_loss: 562.2075 - val_mae: 19.5023\n",
      "Epoch 476/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3755 - mae: 19.5578 - val_loss: 562.2750 - val_mae: 19.4559\n",
      "Epoch 477/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2457 - mae: 19.5479 - val_loss: 563.8635 - val_mae: 19.4878\n",
      "Epoch 478/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0206 - mae: 19.5439 - val_loss: 562.4662 - val_mae: 19.4794\n",
      "Epoch 479/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 567.8211 - mae: 19.5388 - val_loss: 570.0972 - val_mae: 19.7432\n",
      "Epoch 480/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8903 - mae: 19.5706 - val_loss: 561.1157 - val_mae: 19.5167\n",
      "Epoch 481/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0251 - mae: 19.5487 - val_loss: 562.1946 - val_mae: 19.5574\n",
      "Epoch 482/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.6365 - mae: 19.5560 - val_loss: 561.7280 - val_mae: 19.5782\n",
      "Epoch 483/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4475 - mae: 19.5589 - val_loss: 562.5814 - val_mae: 19.5425\n",
      "Epoch 484/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 567.9935 - mae: 19.5416 - val_loss: 560.3864 - val_mae: 19.4937\n",
      "Epoch 485/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 567.7642 - mae: 19.5403 - val_loss: 564.0613 - val_mae: 19.5970\n",
      "Epoch 486/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 567.9887 - mae: 19.5484 - val_loss: 560.9075 - val_mae: 19.4988\n",
      "Epoch 487/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0283 - mae: 19.5411 - val_loss: 565.9543 - val_mae: 19.6801\n",
      "Epoch 488/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0610 - mae: 19.5563 - val_loss: 560.7714 - val_mae: 19.5025\n",
      "Epoch 489/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3533 - mae: 19.5500 - val_loss: 560.3235 - val_mae: 19.4831\n",
      "Epoch 490/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2859 - mae: 19.5581 - val_loss: 561.3171 - val_mae: 19.5161\n",
      "Epoch 491/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.5942 - mae: 19.5573 - val_loss: 565.1401 - val_mae: 19.6461\n",
      "Epoch 492/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1929 - mae: 19.5575 - val_loss: 560.9274 - val_mae: 19.4776\n",
      "Epoch 493/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0079 - mae: 19.5463 - val_loss: 559.7657 - val_mae: 19.4858\n",
      "Epoch 494/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1628 - mae: 19.5587 - val_loss: 561.4674 - val_mae: 19.4734\n",
      "Epoch 495/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 568.4669 - mae: 19.5561 - val_loss: 561.5886 - val_mae: 19.5254\n",
      "Epoch 496/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0818 - mae: 19.5458 - val_loss: 562.0795 - val_mae: 19.5492\n",
      "Epoch 497/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 567.7859 - mae: 19.5448 - val_loss: 561.4783 - val_mae: 19.4809\n",
      "Epoch 498/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1351 - mae: 19.5503 - val_loss: 563.5526 - val_mae: 19.5508\n",
      "Epoch 499/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.0212 - mae: 19.5466 - val_loss: 562.0291 - val_mae: 19.4960\n",
      "Epoch 500/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 568.3092 - mae: 19.5532 - val_loss: 562.0853 - val_mae: 19.5708\n",
      "Epoch 501/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1758 - mae: 19.5604 - val_loss: 561.0764 - val_mae: 19.5172\n",
      "Epoch 502/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 567.9820 - mae: 19.5481 - val_loss: 562.1572 - val_mae: 19.5349\n",
      "Epoch 503/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.4528 - mae: 19.5605 - val_loss: 561.2371 - val_mae: 19.5282\n",
      "Epoch 504/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.6120 - mae: 19.5537 - val_loss: 561.8329 - val_mae: 19.5436\n",
      "Epoch 505/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 568.9762 - mae: 19.5656 - val_loss: 562.0419 - val_mae: 19.5413\n",
      "Epoch 506/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.3293 - mae: 19.5596 - val_loss: 561.5049 - val_mae: 19.5045\n",
      "Epoch 507/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.1663 - mae: 19.5597 - val_loss: 560.4077 - val_mae: 19.5111\n",
      "Epoch 508/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2621 - mae: 19.5516 - val_loss: 561.0863 - val_mae: 19.5027\n",
      "Epoch 509/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2209 - mae: 19.5527 - val_loss: 560.1907 - val_mae: 19.4854\n",
      "Epoch 510/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 567.8630 - mae: 19.5412 - val_loss: 561.5314 - val_mae: 19.5448\n",
      "Epoch 511/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.1238 - mae: 19.5498 - val_loss: 562.9069 - val_mae: 19.5418\n",
      "Epoch 512/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7281 - mae: 19.5613 - val_loss: 561.2807 - val_mae: 19.5155\n",
      "Epoch 513/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7023 - mae: 19.5638 - val_loss: 560.2128 - val_mae: 19.4780\n",
      "Epoch 514/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9864 - mae: 19.5694 - val_loss: 560.9130 - val_mae: 19.4974\n",
      "Epoch 515/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.4814 - mae: 19.5671 - val_loss: 562.8540 - val_mae: 19.6006\n",
      "Epoch 516/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 568.8818 - mae: 19.5686 - val_loss: 560.6492 - val_mae: 19.5020\n",
      "Epoch 517/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.3504 - mae: 19.5486 - val_loss: 562.5142 - val_mae: 19.5413\n",
      "Epoch 518/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.2289 - mae: 19.5514 - val_loss: 560.7213 - val_mae: 19.4776\n",
      "Epoch 519/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 574.3952 - mae: 19.6257 - val_loss: 560.5434 - val_mae: 19.5368\n",
      "Epoch 520/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.8672 - mae: 19.5886 - val_loss: 560.7733 - val_mae: 19.4800\n",
      "Epoch 521/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.2282 - mae: 19.5807 - val_loss: 562.3056 - val_mae: 19.6291\n",
      "Epoch 522/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.5224 - mae: 19.5785 - val_loss: 560.5205 - val_mae: 19.5235\n",
      "Epoch 523/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.3451 - mae: 19.5541 - val_loss: 566.7808 - val_mae: 19.7450\n",
      "Epoch 524/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.5142 - mae: 19.5554 - val_loss: 562.9687 - val_mae: 19.5309\n",
      "Epoch 525/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.2821 - mae: 19.5646 - val_loss: 560.3699 - val_mae: 19.4510\n",
      "Epoch 526/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.1052 - mae: 19.5597 - val_loss: 561.3124 - val_mae: 19.5420\n",
      "Epoch 527/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.2490 - mae: 19.5604 - val_loss: 559.7072 - val_mae: 19.4471\n",
      "Epoch 528/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 570.6047 - mae: 19.5692 - val_loss: 559.3593 - val_mae: 19.4872\n",
      "Epoch 529/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.7144 - mae: 19.5691 - val_loss: 559.8967 - val_mae: 19.5284\n",
      "Epoch 530/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.7984 - mae: 19.5781 - val_loss: 559.4911 - val_mae: 19.4863\n",
      "Epoch 531/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.5737 - mae: 19.5681 - val_loss: 559.3754 - val_mae: 19.5031\n",
      "Epoch 532/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.3199 - mae: 19.5682 - val_loss: 559.9734 - val_mae: 19.5077\n",
      "Epoch 533/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8747 - mae: 19.5611 - val_loss: 559.9904 - val_mae: 19.4695\n",
      "Epoch 534/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9572 - mae: 19.5588 - val_loss: 559.6298 - val_mae: 19.4970\n",
      "Epoch 535/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.0545 - mae: 19.5619 - val_loss: 560.0117 - val_mae: 19.4884\n",
      "Epoch 536/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.1260 - mae: 19.5580 - val_loss: 560.3904 - val_mae: 19.5031\n",
      "Epoch 537/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9286 - mae: 19.5641 - val_loss: 561.2196 - val_mae: 19.5299\n",
      "Epoch 538/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8173 - mae: 19.5656 - val_loss: 560.3464 - val_mae: 19.5056\n",
      "Epoch 539/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8870 - mae: 19.5689 - val_loss: 560.2007 - val_mae: 19.4859\n",
      "Epoch 540/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.1381 - mae: 19.5636 - val_loss: 560.2233 - val_mae: 19.5103\n",
      "Epoch 541/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.4174 - mae: 19.5592 - val_loss: 560.9704 - val_mae: 19.5353\n",
      "Epoch 542/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7414 - mae: 19.5689 - val_loss: 560.0629 - val_mae: 19.4976\n",
      "Epoch 543/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8671 - mae: 19.5625 - val_loss: 560.1716 - val_mae: 19.5078\n",
      "Epoch 544/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9200 - mae: 19.5635 - val_loss: 559.9459 - val_mae: 19.5048\n",
      "Epoch 545/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7678 - mae: 19.5611 - val_loss: 559.6788 - val_mae: 19.4997\n",
      "Epoch 546/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.2443 - mae: 19.5665 - val_loss: 559.4504 - val_mae: 19.4944\n",
      "Epoch 547/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.9321 - mae: 19.5634 - val_loss: 560.9786 - val_mae: 19.5136\n",
      "Epoch 548/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7552 - mae: 19.5551 - val_loss: 560.3215 - val_mae: 19.5224\n",
      "Epoch 549/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8373 - mae: 19.5707 - val_loss: 559.8595 - val_mae: 19.4907\n",
      "Epoch 550/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8505 - mae: 19.5630 - val_loss: 560.0739 - val_mae: 19.4986\n",
      "Epoch 551/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 569.0252 - mae: 19.5622 - val_loss: 559.6783 - val_mae: 19.5029\n",
      "Epoch 552/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.8024 - mae: 19.5620 - val_loss: 560.5046 - val_mae: 19.5144\n",
      "Epoch 553/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 568.7067 - mae: 19.5589 - val_loss: 559.6736 - val_mae: 19.4962\n",
      "Epoch 554/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 568.8074 - mae: 19.5649 - val_loss: 559.7236 - val_mae: 19.4884\n",
      "Epoch 555/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.5325 - mae: 19.5699 - val_loss: 559.5067 - val_mae: 19.5003\n",
      "Epoch 556/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6955 - mae: 19.5756 - val_loss: 559.5033 - val_mae: 19.5000\n",
      "Epoch 557/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6200 - mae: 19.5790 - val_loss: 559.3868 - val_mae: 19.4781\n",
      "Epoch 558/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7209 - mae: 19.5734 - val_loss: 559.4103 - val_mae: 19.4884\n",
      "Epoch 559/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6960 - mae: 19.5771 - val_loss: 559.4142 - val_mae: 19.4892\n",
      "Epoch 560/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6469 - mae: 19.5761 - val_loss: 559.4036 - val_mae: 19.4872\n",
      "Epoch 561/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7003 - mae: 19.5756 - val_loss: 559.3995 - val_mae: 19.4861\n",
      "Epoch 562/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6586 - mae: 19.5711 - val_loss: 559.5984 - val_mae: 19.5078\n",
      "Epoch 563/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6920 - mae: 19.5793 - val_loss: 559.4949 - val_mae: 19.4992\n",
      "Epoch 564/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6331 - mae: 19.5812 - val_loss: 559.3881 - val_mae: 19.4823\n",
      "Epoch 565/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6324 - mae: 19.5684 - val_loss: 559.5528 - val_mae: 19.5043\n",
      "Epoch 566/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6952 - mae: 19.5799 - val_loss: 559.4477 - val_mae: 19.4940\n",
      "Epoch 567/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6221 - mae: 19.5696 - val_loss: 559.6920 - val_mae: 19.5141\n",
      "Epoch 568/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6240 - mae: 19.5850 - val_loss: 559.3868 - val_mae: 19.4778\n",
      "Epoch 569/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6228 - mae: 19.5675 - val_loss: 559.6337 - val_mae: 19.5103\n",
      "Epoch 570/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7009 - mae: 19.5825 - val_loss: 559.4330 - val_mae: 19.4920\n",
      "Epoch 571/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7191 - mae: 19.5720 - val_loss: 559.4879 - val_mae: 19.4984\n",
      "Epoch 572/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7299 - mae: 19.5796 - val_loss: 559.4528 - val_mae: 19.4946\n",
      "Epoch 573/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6607 - mae: 19.5757 - val_loss: 559.4767 - val_mae: 19.4973\n",
      "Epoch 574/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 569.6599 - mae: 19.5770 - val_loss: 559.4233 - val_mae: 19.4907\n",
      "Epoch 575/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 569.6931 - mae: 19.5720 - val_loss: 559.5101 - val_mae: 19.5006\n",
      "Epoch 576/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6277 - mae: 19.5809 - val_loss: 559.3870 - val_mae: 19.4812\n",
      "Epoch 577/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7031 - mae: 19.5709 - val_loss: 559.4929 - val_mae: 19.4989\n",
      "Epoch 578/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6851 - mae: 19.5779 - val_loss: 559.4391 - val_mae: 19.4929\n",
      "Epoch 579/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6859 - mae: 19.5758 - val_loss: 559.4730 - val_mae: 19.4969\n",
      "Epoch 580/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6445 - mae: 19.5697 - val_loss: 559.7290 - val_mae: 19.5165\n",
      "Epoch 581/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6867 - mae: 19.5796 - val_loss: 559.5849 - val_mae: 19.5068\n",
      "Epoch 582/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6547 - mae: 19.5786 - val_loss: 559.4266 - val_mae: 19.4911\n",
      "Epoch 583/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6903 - mae: 19.5720 - val_loss: 559.5031 - val_mae: 19.5000\n",
      "Epoch 584/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6918 - mae: 19.5805 - val_loss: 559.4119 - val_mae: 19.4887\n",
      "Epoch 585/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6744 - mae: 19.5716 - val_loss: 559.5311 - val_mae: 19.5025\n",
      "Epoch 586/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7139 - mae: 19.5786 - val_loss: 559.4780 - val_mae: 19.4974\n",
      "Epoch 587/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6556 - mae: 19.5786 - val_loss: 559.4150 - val_mae: 19.4893\n",
      "Epoch 588/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6784 - mae: 19.5766 - val_loss: 559.4169 - val_mae: 19.4896\n",
      "Epoch 589/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6548 - mae: 19.5695 - val_loss: 559.6191 - val_mae: 19.5093\n",
      "Epoch 590/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 569.6935 - mae: 19.5813 - val_loss: 559.4703 - val_mae: 19.4967\n",
      "Epoch 591/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.5836 - mae: 19.5673 - val_loss: 559.7657 - val_mae: 19.5187\n",
      "Epoch 592/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6841 - mae: 19.5811 - val_loss: 559.4730 - val_mae: 19.4969\n",
      "Epoch 593/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6354 - mae: 19.5702 - val_loss: 559.7239 - val_mae: 19.5161\n",
      "Epoch 594/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6873 - mae: 19.5831 - val_loss: 559.3989 - val_mae: 19.4859\n",
      "Epoch 595/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6728 - mae: 19.5738 - val_loss: 559.4804 - val_mae: 19.4977\n",
      "Epoch 596/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6657 - mae: 19.5713 - val_loss: 559.5552 - val_mae: 19.5045\n",
      "Epoch 597/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6657 - mae: 19.5798 - val_loss: 559.4199 - val_mae: 19.4901\n",
      "Epoch 598/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6918 - mae: 19.5762 - val_loss: 559.4273 - val_mae: 19.4913\n",
      "Epoch 599/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6915 - mae: 19.5732 - val_loss: 559.4750 - val_mae: 19.4971\n",
      "Epoch 600/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6699 - mae: 19.5765 - val_loss: 559.4330 - val_mae: 19.4921\n",
      "Epoch 601/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6881 - mae: 19.5716 - val_loss: 559.5936 - val_mae: 19.5074\n",
      "Epoch 602/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6887 - mae: 19.5781 - val_loss: 559.5198 - val_mae: 19.5015\n",
      "Epoch 603/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6845 - mae: 19.5769 - val_loss: 559.5163 - val_mae: 19.5012\n",
      "Epoch 604/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 569.6771 - mae: 19.5800 - val_loss: 559.4088 - val_mae: 19.4882\n",
      "Epoch 605/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 569.7138 - mae: 19.5745 - val_loss: 559.4824 - val_mae: 19.4979\n",
      "Epoch 606/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6683 - mae: 19.5722 - val_loss: 559.5691 - val_mae: 19.5056\n",
      "Epoch 607/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6632 - mae: 19.5773 - val_loss: 559.4736 - val_mae: 19.4970\n",
      "Epoch 608/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6555 - mae: 19.5791 - val_loss: 559.4092 - val_mae: 19.4884\n",
      "Epoch 609/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6985 - mae: 19.5744 - val_loss: 559.4633 - val_mae: 19.4959\n",
      "Epoch 610/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6862 - mae: 19.5756 - val_loss: 559.4855 - val_mae: 19.4982\n",
      "Epoch 611/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7068 - mae: 19.5814 - val_loss: 559.3994 - val_mae: 19.4862\n",
      "Epoch 612/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6570 - mae: 19.5801 - val_loss: 559.3873 - val_mae: 19.4818\n",
      "Epoch 613/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7513 - mae: 19.5712 - val_loss: 559.4570 - val_mae: 19.4951\n",
      "Epoch 614/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6705 - mae: 19.5733 - val_loss: 559.5864 - val_mae: 19.5069\n",
      "Epoch 615/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6823 - mae: 19.5789 - val_loss: 559.4252 - val_mae: 19.4910\n",
      "Epoch 616/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6624 - mae: 19.5738 - val_loss: 559.6075 - val_mae: 19.5084\n",
      "Epoch 617/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6596 - mae: 19.5769 - val_loss: 559.4600 - val_mae: 19.4955\n",
      "Epoch 618/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6963 - mae: 19.5711 - val_loss: 559.5777 - val_mae: 19.5062\n",
      "Epoch 619/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6609 - mae: 19.5808 - val_loss: 559.4107 - val_mae: 19.4886\n",
      "Epoch 620/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6802 - mae: 19.5703 - val_loss: 559.5190 - val_mae: 19.5014\n",
      "Epoch 621/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6337 - mae: 19.5814 - val_loss: 559.3907 - val_mae: 19.4835\n",
      "Epoch 622/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6260 - mae: 19.5672 - val_loss: 559.6166 - val_mae: 19.5091\n",
      "Epoch 623/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6850 - mae: 19.5774 - val_loss: 559.4885 - val_mae: 19.4985\n",
      "Epoch 624/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6785 - mae: 19.5782 - val_loss: 559.4581 - val_mae: 19.4952\n",
      "Epoch 625/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6622 - mae: 19.5710 - val_loss: 559.6566 - val_mae: 19.5118\n",
      "Epoch 626/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6941 - mae: 19.5765 - val_loss: 559.6016 - val_mae: 19.5080\n",
      "Epoch 627/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6464 - mae: 19.5707 - val_loss: 559.9216 - val_mae: 19.5274\n",
      "Epoch 628/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6891 - mae: 19.5859 - val_loss: 559.4444 - val_mae: 19.4936\n",
      "Epoch 629/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6943 - mae: 19.5753 - val_loss: 559.4651 - val_mae: 19.4960\n",
      "Epoch 630/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6464 - mae: 19.5764 - val_loss: 559.4241 - val_mae: 19.4907\n",
      "Epoch 631/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7039 - mae: 19.5730 - val_loss: 559.5391 - val_mae: 19.5031\n",
      "Epoch 632/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6480 - mae: 19.5762 - val_loss: 559.4697 - val_mae: 19.4965\n",
      "Epoch 633/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6932 - mae: 19.5760 - val_loss: 559.5422 - val_mae: 19.5034\n",
      "Epoch 634/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6677 - mae: 19.5780 - val_loss: 559.4034 - val_mae: 19.4870\n",
      "Epoch 635/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7169 - mae: 19.5746 - val_loss: 559.4658 - val_mae: 19.4961\n",
      "Epoch 636/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6900 - mae: 19.5747 - val_loss: 559.4891 - val_mae: 19.4986\n",
      "Epoch 637/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6951 - mae: 19.5770 - val_loss: 559.5194 - val_mae: 19.5014\n",
      "Epoch 638/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6920 - mae: 19.5781 - val_loss: 559.4962 - val_mae: 19.4993\n",
      "Epoch 639/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6960 - mae: 19.5766 - val_loss: 559.4512 - val_mae: 19.4944\n",
      "Epoch 640/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6977 - mae: 19.5746 - val_loss: 559.4300 - val_mae: 19.4916\n",
      "Epoch 641/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6401 - mae: 19.5695 - val_loss: 559.7696 - val_mae: 19.5189\n",
      "Epoch 642/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6790 - mae: 19.5820 - val_loss: 559.4637 - val_mae: 19.4959\n",
      "Epoch 643/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7198 - mae: 19.5754 - val_loss: 559.5172 - val_mae: 19.5012\n",
      "Epoch 644/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6769 - mae: 19.5784 - val_loss: 559.4322 - val_mae: 19.4919\n",
      "Epoch 645/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6888 - mae: 19.5756 - val_loss: 559.4442 - val_mae: 19.4936\n",
      "Epoch 646/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6503 - mae: 19.5718 - val_loss: 559.5585 - val_mae: 19.5048\n",
      "Epoch 647/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6953 - mae: 19.5766 - val_loss: 559.4842 - val_mae: 19.4981\n",
      "Epoch 648/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6830 - mae: 19.5728 - val_loss: 559.6094 - val_mae: 19.5086\n",
      "Epoch 649/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6070 - mae: 19.5815 - val_loss: 559.4127 - val_mae: 19.4889\n",
      "Epoch 650/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7015 - mae: 19.5732 - val_loss: 559.4769 - val_mae: 19.4973\n",
      "Epoch 651/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6656 - mae: 19.5729 - val_loss: 559.5662 - val_mae: 19.5053\n",
      "Epoch 652/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6893 - mae: 19.5805 - val_loss: 559.4409 - val_mae: 19.4932\n",
      "Epoch 653/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6802 - mae: 19.5768 - val_loss: 559.4338 - val_mae: 19.4922\n",
      "Epoch 654/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7198 - mae: 19.5744 - val_loss: 559.4473 - val_mae: 19.4940\n",
      "Epoch 655/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6971 - mae: 19.5736 - val_loss: 559.5364 - val_mae: 19.5029\n",
      "Epoch 656/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6736 - mae: 19.5741 - val_loss: 559.5688 - val_mae: 19.5055\n",
      "Epoch 657/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6169 - mae: 19.5807 - val_loss: 559.3904 - val_mae: 19.4834\n",
      "Epoch 658/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7008 - mae: 19.5701 - val_loss: 559.4821 - val_mae: 19.4979\n",
      "Epoch 659/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6891 - mae: 19.5787 - val_loss: 559.4608 - val_mae: 19.4956\n",
      "Epoch 660/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6671 - mae: 19.5739 - val_loss: 559.5112 - val_mae: 19.5007\n",
      "Epoch 661/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6620 - mae: 19.5733 - val_loss: 559.6069 - val_mae: 19.5084\n",
      "Epoch 662/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.7312 - mae: 19.5780 - val_loss: 559.5883 - val_mae: 19.5070\n",
      "Epoch 663/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7172 - mae: 19.5762 - val_loss: 559.6124 - val_mae: 19.5088\n",
      "Epoch 664/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6729 - mae: 19.5768 - val_loss: 559.6165 - val_mae: 19.5091\n",
      "Epoch 665/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6524 - mae: 19.5827 - val_loss: 559.4111 - val_mae: 19.4887\n",
      "Epoch 666/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6894 - mae: 19.5748 - val_loss: 559.4233 - val_mae: 19.4906\n",
      "Epoch 667/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6033 - mae: 19.5659 - val_loss: 559.7454 - val_mae: 19.5174\n",
      "Epoch 668/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6980 - mae: 19.5799 - val_loss: 559.5484 - val_mae: 19.5040\n",
      "Epoch 669/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6920 - mae: 19.5786 - val_loss: 559.4720 - val_mae: 19.4968\n",
      "Epoch 670/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6867 - mae: 19.5765 - val_loss: 559.4276 - val_mae: 19.4913\n",
      "Epoch 671/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.5730 - mae: 19.5645 - val_loss: 559.8782 - val_mae: 19.5250\n",
      "Epoch 672/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7205 - mae: 19.5848 - val_loss: 559.4918 - val_mae: 19.4988\n",
      "Epoch 673/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6821 - mae: 19.5770 - val_loss: 559.4182 - val_mae: 19.4899\n",
      "Epoch 674/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6304 - mae: 19.5762 - val_loss: 559.3860 - val_mae: 19.4792\n",
      "Epoch 675/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7140 - mae: 19.5711 - val_loss: 559.4480 - val_mae: 19.4940\n",
      "Epoch 676/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6836 - mae: 19.5757 - val_loss: 559.4843 - val_mae: 19.4981\n",
      "Epoch 677/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6302 - mae: 19.5707 - val_loss: 559.8140 - val_mae: 19.5215\n",
      "Epoch 678/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7114 - mae: 19.5840 - val_loss: 559.4360 - val_mae: 19.4925\n",
      "Epoch 679/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7132 - mae: 19.5746 - val_loss: 559.4728 - val_mae: 19.4969\n",
      "Epoch 680/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6706 - mae: 19.5754 - val_loss: 559.4877 - val_mae: 19.4984\n",
      "Epoch 681/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6455 - mae: 19.5712 - val_loss: 559.6956 - val_mae: 19.5144\n",
      "Epoch 682/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7271 - mae: 19.5784 - val_loss: 559.5613 - val_mae: 19.5050\n",
      "Epoch 683/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6667 - mae: 19.5765 - val_loss: 559.5153 - val_mae: 19.5011\n",
      "Epoch 684/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6746 - mae: 19.5766 - val_loss: 559.5031 - val_mae: 19.5000\n",
      "Epoch 685/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6942 - mae: 19.5738 - val_loss: 559.4960 - val_mae: 19.4993\n",
      "Epoch 686/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6942 - mae: 19.5801 - val_loss: 559.4118 - val_mae: 19.4887\n",
      "Epoch 687/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6777 - mae: 19.5710 - val_loss: 559.5845 - val_mae: 19.5067\n",
      "Epoch 688/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6887 - mae: 19.5792 - val_loss: 559.4528 - val_mae: 19.4946\n",
      "Epoch 689/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6509 - mae: 19.5776 - val_loss: 559.4055 - val_mae: 19.4875\n",
      "Epoch 690/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6465 - mae: 19.5710 - val_loss: 559.5961 - val_mae: 19.5076\n",
      "Epoch 691/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6956 - mae: 19.5792 - val_loss: 559.4153 - val_mae: 19.4894\n",
      "Epoch 692/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6382 - mae: 19.5698 - val_loss: 559.7847 - val_mae: 19.5198\n",
      "Epoch 693/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6819 - mae: 19.5819 - val_loss: 559.4457 - val_mae: 19.4938\n",
      "Epoch 694/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6879 - mae: 19.5684 - val_loss: 559.6642 - val_mae: 19.5124\n",
      "Epoch 695/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7140 - mae: 19.5798 - val_loss: 559.5413 - val_mae: 19.5033\n",
      "Epoch 696/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6798 - mae: 19.5762 - val_loss: 559.5903 - val_mae: 19.5072\n",
      "Epoch 697/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7062 - mae: 19.5772 - val_loss: 559.5701 - val_mae: 19.5056\n",
      "Epoch 698/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6918 - mae: 19.5764 - val_loss: 559.6038 - val_mae: 19.5082\n",
      "Epoch 699/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6492 - mae: 19.5802 - val_loss: 559.4631 - val_mae: 19.4958\n",
      "Epoch 700/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6927 - mae: 19.5751 - val_loss: 559.5377 - val_mae: 19.5030\n",
      "Epoch 701/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6754 - mae: 19.5771 - val_loss: 559.4758 - val_mae: 19.4972\n",
      "Epoch 702/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6685 - mae: 19.5733 - val_loss: 559.5741 - val_mae: 19.5060\n",
      "Epoch 703/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6613 - mae: 19.5781 - val_loss: 559.4481 - val_mae: 19.4940\n",
      "Epoch 704/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6829 - mae: 19.5756 - val_loss: 559.4607 - val_mae: 19.4956\n",
      "Epoch 705/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6563 - mae: 19.5732 - val_loss: 559.4829 - val_mae: 19.4980\n",
      "Epoch 706/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6614 - mae: 19.5781 - val_loss: 559.3945 - val_mae: 19.4848\n",
      "Epoch 707/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6714 - mae: 19.5761 - val_loss: 559.3942 - val_mae: 19.4847\n",
      "Epoch 708/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6873 - mae: 19.5720 - val_loss: 559.4739 - val_mae: 19.4970\n",
      "Epoch 709/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6562 - mae: 19.5758 - val_loss: 559.4631 - val_mae: 19.4958\n",
      "Epoch 710/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6608 - mae: 19.5751 - val_loss: 559.4901 - val_mae: 19.4987\n",
      "Epoch 711/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6693 - mae: 19.5773 - val_loss: 559.4274 - val_mae: 19.4913\n",
      "Epoch 712/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6806 - mae: 19.5729 - val_loss: 559.4268 - val_mae: 19.4912\n",
      "Epoch 713/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6733 - mae: 19.5792 - val_loss: 559.4241 - val_mae: 19.4907\n",
      "Epoch 714/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6917 - mae: 19.5731 - val_loss: 559.4485 - val_mae: 19.4941\n",
      "Epoch 715/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 569.6846 - mae: 19.5794 - val_loss: 559.4421 - val_mae: 19.4933\n",
      "Epoch 716/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6987 - mae: 19.5719 - val_loss: 559.5505 - val_mae: 19.5041\n",
      "Epoch 717/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6602 - mae: 19.5723 - val_loss: 559.7081 - val_mae: 19.5151\n",
      "Epoch 718/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6826 - mae: 19.5762 - val_loss: 559.5670 - val_mae: 19.5054\n",
      "Epoch 719/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7009 - mae: 19.5795 - val_loss: 559.4534 - val_mae: 19.4947\n",
      "Epoch 720/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7153 - mae: 19.5758 - val_loss: 559.4854 - val_mae: 19.4982\n",
      "Epoch 721/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.5918 - mae: 19.5790 - val_loss: 559.3875 - val_mae: 19.4818\n",
      "Epoch 722/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7269 - mae: 19.5733 - val_loss: 559.4291 - val_mae: 19.4915\n",
      "Epoch 723/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6832 - mae: 19.5757 - val_loss: 559.4885 - val_mae: 19.4985\n",
      "Epoch 724/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6460 - mae: 19.5779 - val_loss: 559.3920 - val_mae: 19.4840\n",
      "Epoch 725/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6889 - mae: 19.5712 - val_loss: 559.5140 - val_mae: 19.5009\n",
      "Epoch 726/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6843 - mae: 19.5728 - val_loss: 559.5887 - val_mae: 19.5071\n",
      "Epoch 727/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6904 - mae: 19.5763 - val_loss: 559.6217 - val_mae: 19.5094\n",
      "Epoch 728/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6768 - mae: 19.5804 - val_loss: 559.4254 - val_mae: 19.4909\n",
      "Epoch 729/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6822 - mae: 19.5747 - val_loss: 559.4398 - val_mae: 19.4930\n",
      "Epoch 730/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6825 - mae: 19.5746 - val_loss: 559.4878 - val_mae: 19.4984\n",
      "Epoch 731/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6711 - mae: 19.5752 - val_loss: 559.6218 - val_mae: 19.5095\n",
      "Epoch 732/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6308 - mae: 19.5828 - val_loss: 559.3911 - val_mae: 19.4837\n",
      "Epoch 733/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6959 - mae: 19.5747 - val_loss: 559.4117 - val_mae: 19.4888\n",
      "Epoch 734/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6142 - mae: 19.5684 - val_loss: 559.7381 - val_mae: 19.5170\n",
      "Epoch 735/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6985 - mae: 19.5790 - val_loss: 559.5428 - val_mae: 19.5035\n",
      "Epoch 736/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6714 - mae: 19.5764 - val_loss: 559.5291 - val_mae: 19.5023\n",
      "Epoch 737/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7068 - mae: 19.5766 - val_loss: 559.5955 - val_mae: 19.5076\n",
      "Epoch 738/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6711 - mae: 19.5806 - val_loss: 559.4059 - val_mae: 19.4876\n",
      "Epoch 739/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6985 - mae: 19.5741 - val_loss: 559.4667 - val_mae: 19.4962\n",
      "Epoch 740/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7103 - mae: 19.5780 - val_loss: 559.4517 - val_mae: 19.4945\n",
      "Epoch 741/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.5932 - mae: 19.5776 - val_loss: 559.3925 - val_mae: 19.4842\n",
      "Epoch 742/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6691 - mae: 19.5700 - val_loss: 559.5222 - val_mae: 19.5017\n",
      "Epoch 743/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6718 - mae: 19.5756 - val_loss: 559.5481 - val_mae: 19.5039\n",
      "Epoch 744/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6986 - mae: 19.5781 - val_loss: 559.4429 - val_mae: 19.4933\n",
      "Epoch 745/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6445 - mae: 19.5753 - val_loss: 559.5018 - val_mae: 19.4998\n",
      "Epoch 746/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6219 - mae: 19.5765 - val_loss: 559.4552 - val_mae: 19.4949\n",
      "Epoch 747/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6829 - mae: 19.5731 - val_loss: 559.5723 - val_mae: 19.5058\n",
      "Epoch 748/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6436 - mae: 19.5728 - val_loss: 559.6031 - val_mae: 19.5081\n",
      "Epoch 749/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6379 - mae: 19.5708 - val_loss: 559.7349 - val_mae: 19.5168\n",
      "Epoch 750/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7366 - mae: 19.5849 - val_loss: 559.4595 - val_mae: 19.4954\n",
      "Epoch 751/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6328 - mae: 19.5712 - val_loss: 559.4766 - val_mae: 19.4973\n",
      "Epoch 752/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6722 - mae: 19.5740 - val_loss: 559.6246 - val_mae: 19.5097\n",
      "Epoch 753/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6730 - mae: 19.5749 - val_loss: 559.5942 - val_mae: 19.5075\n",
      "Epoch 754/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6520 - mae: 19.5800 - val_loss: 559.3986 - val_mae: 19.4859\n",
      "Epoch 755/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6646 - mae: 19.5770 - val_loss: 559.3901 - val_mae: 19.4833\n",
      "Epoch 756/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6315 - mae: 19.5660 - val_loss: 559.7108 - val_mae: 19.5153\n",
      "Epoch 757/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6600 - mae: 19.5760 - val_loss: 559.7716 - val_mae: 19.5190\n",
      "Epoch 758/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7097 - mae: 19.5834 - val_loss: 559.4752 - val_mae: 19.4972\n",
      "Epoch 759/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7040 - mae: 19.5765 - val_loss: 559.4492 - val_mae: 19.4942\n",
      "Epoch 760/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6744 - mae: 19.5747 - val_loss: 559.4420 - val_mae: 19.4933\n",
      "Epoch 761/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6870 - mae: 19.5775 - val_loss: 559.4306 - val_mae: 19.4918\n",
      "Epoch 762/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6856 - mae: 19.5721 - val_loss: 559.4963 - val_mae: 19.4993\n",
      "Epoch 763/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6689 - mae: 19.5754 - val_loss: 559.4535 - val_mae: 19.4947\n",
      "Epoch 764/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6893 - mae: 19.5771 - val_loss: 559.4628 - val_mae: 19.4958\n",
      "Epoch 765/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6879 - mae: 19.5776 - val_loss: 559.4074 - val_mae: 19.4879\n",
      "Epoch 766/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6657 - mae: 19.5743 - val_loss: 559.4348 - val_mae: 19.4923\n",
      "Epoch 767/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6241 - mae: 19.5786 - val_loss: 559.3860 - val_mae: 19.4788\n",
      "Epoch 768/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6356 - mae: 19.5715 - val_loss: 559.4422 - val_mae: 19.4933\n",
      "Epoch 769/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6907 - mae: 19.5724 - val_loss: 559.5228 - val_mae: 19.5018\n",
      "Epoch 770/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6692 - mae: 19.5737 - val_loss: 559.5666 - val_mae: 19.5054\n",
      "Epoch 771/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6834 - mae: 19.5754 - val_loss: 559.6160 - val_mae: 19.5091\n",
      "Epoch 772/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6669 - mae: 19.5745 - val_loss: 559.6083 - val_mae: 19.5085\n",
      "Epoch 773/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7003 - mae: 19.5755 - val_loss: 559.6651 - val_mae: 19.5124\n",
      "Epoch 774/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7248 - mae: 19.5812 - val_loss: 559.4775 - val_mae: 19.4974\n",
      "Epoch 775/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6761 - mae: 19.5773 - val_loss: 559.4451 - val_mae: 19.4936\n",
      "Epoch 776/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6672 - mae: 19.5746 - val_loss: 559.4243 - val_mae: 19.4908\n",
      "Epoch 777/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6039 - mae: 19.5678 - val_loss: 559.8071 - val_mae: 19.5211\n",
      "Epoch 778/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 569.7173 - mae: 19.5795 - val_loss: 559.5400 - val_mae: 19.5032\n",
      "Epoch 779/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6904 - mae: 19.5760 - val_loss: 559.5518 - val_mae: 19.5042\n",
      "Epoch 780/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6707 - mae: 19.5810 - val_loss: 559.3919 - val_mae: 19.4839\n",
      "Epoch 781/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6802 - mae: 19.5758 - val_loss: 559.4272 - val_mae: 19.4913\n",
      "Epoch 782/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6700 - mae: 19.5707 - val_loss: 559.6501 - val_mae: 19.5114\n",
      "Epoch 783/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6664 - mae: 19.5789 - val_loss: 559.4429 - val_mae: 19.4934\n",
      "Epoch 784/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6667 - mae: 19.5709 - val_loss: 559.6662 - val_mae: 19.5124\n",
      "Epoch 785/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6827 - mae: 19.5820 - val_loss: 559.4225 - val_mae: 19.4905\n",
      "Epoch 786/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7054 - mae: 19.5741 - val_loss: 559.4890 - val_mae: 19.4986\n",
      "Epoch 787/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6972 - mae: 19.5757 - val_loss: 559.4679 - val_mae: 19.4964\n",
      "Epoch 788/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6725 - mae: 19.5734 - val_loss: 559.6456 - val_mae: 19.5111\n",
      "Epoch 789/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6876 - mae: 19.5762 - val_loss: 559.5925 - val_mae: 19.5073\n",
      "Epoch 790/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6993 - mae: 19.5801 - val_loss: 559.4830 - val_mae: 19.4980\n",
      "Epoch 791/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.7065 - mae: 19.5781 - val_loss: 559.4600 - val_mae: 19.4955\n",
      "Epoch 792/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6917 - mae: 19.5746 - val_loss: 559.4887 - val_mae: 19.4985\n",
      "Epoch 793/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6978 - mae: 19.5781 - val_loss: 559.4224 - val_mae: 19.4905\n",
      "Epoch 794/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6873 - mae: 19.5751 - val_loss: 559.4257 - val_mae: 19.4910\n",
      "Epoch 795/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6978 - mae: 19.5749 - val_loss: 559.4419 - val_mae: 19.4933\n",
      "Epoch 796/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6555 - mae: 19.5728 - val_loss: 559.4987 - val_mae: 19.4995\n",
      "Epoch 797/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6768 - mae: 19.5722 - val_loss: 559.6672 - val_mae: 19.5125\n",
      "Epoch 798/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6192 - mae: 19.5842 - val_loss: 559.3859 - val_mae: 19.4801\n",
      "Epoch 799/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6774 - mae: 19.5720 - val_loss: 559.4084 - val_mae: 19.4881\n",
      "Epoch 800/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6689 - mae: 19.5762 - val_loss: 559.4135 - val_mae: 19.4890\n",
      "Epoch 801/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6834 - mae: 19.5745 - val_loss: 559.4775 - val_mae: 19.4974\n",
      "Epoch 802/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6979 - mae: 19.5753 - val_loss: 559.4618 - val_mae: 19.4957\n",
      "Epoch 803/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6326 - mae: 19.5705 - val_loss: 559.7116 - val_mae: 19.5154\n",
      "Epoch 804/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7219 - mae: 19.5793 - val_loss: 559.5616 - val_mae: 19.5050\n",
      "Epoch 805/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6620 - mae: 19.5749 - val_loss: 559.5400 - val_mae: 19.5032\n",
      "Epoch 806/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6351 - mae: 19.5761 - val_loss: 559.4977 - val_mae: 19.4994\n",
      "Epoch 807/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7050 - mae: 19.5772 - val_loss: 559.4521 - val_mae: 19.4945\n",
      "Epoch 808/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6854 - mae: 19.5740 - val_loss: 559.5740 - val_mae: 19.5059\n",
      "Epoch 809/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6700 - mae: 19.5748 - val_loss: 559.6309 - val_mae: 19.5101\n",
      "Epoch 810/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6806 - mae: 19.5788 - val_loss: 559.4586 - val_mae: 19.4953\n",
      "Epoch 811/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6851 - mae: 19.5762 - val_loss: 559.4634 - val_mae: 19.4959\n",
      "Epoch 812/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6881 - mae: 19.5746 - val_loss: 559.4843 - val_mae: 19.4981\n",
      "Epoch 813/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6675 - mae: 19.5706 - val_loss: 559.7089 - val_mae: 19.5152\n",
      "Epoch 814/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6570 - mae: 19.5819 - val_loss: 559.3935 - val_mae: 19.4845\n",
      "Epoch 815/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6342 - mae: 19.5666 - val_loss: 559.7062 - val_mae: 19.5150\n",
      "Epoch 816/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7277 - mae: 19.5785 - val_loss: 559.5284 - val_mae: 19.5022\n",
      "Epoch 817/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6858 - mae: 19.5771 - val_loss: 559.4559 - val_mae: 19.4950\n",
      "Epoch 818/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6884 - mae: 19.5729 - val_loss: 559.5392 - val_mae: 19.5032\n",
      "Epoch 819/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6786 - mae: 19.5805 - val_loss: 559.4261 - val_mae: 19.4911\n",
      "Epoch 820/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6701 - mae: 19.5775 - val_loss: 559.4092 - val_mae: 19.4882\n",
      "Epoch 821/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6742 - mae: 19.5720 - val_loss: 559.4706 - val_mae: 19.4967\n",
      "Epoch 822/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7068 - mae: 19.5748 - val_loss: 559.5407 - val_mae: 19.5033\n",
      "Epoch 823/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6580 - mae: 19.5727 - val_loss: 559.6381 - val_mae: 19.5106\n",
      "Epoch 824/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6848 - mae: 19.5784 - val_loss: 559.6528 - val_mae: 19.5116\n",
      "Epoch 825/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6980 - mae: 19.5801 - val_loss: 559.5149 - val_mae: 19.5010\n",
      "Epoch 826/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6639 - mae: 19.5747 - val_loss: 559.4844 - val_mae: 19.4981\n",
      "Epoch 827/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6678 - mae: 19.5797 - val_loss: 559.4075 - val_mae: 19.4879\n",
      "Epoch 828/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6655 - mae: 19.5775 - val_loss: 559.3890 - val_mae: 19.4827\n",
      "Epoch 829/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6556 - mae: 19.5673 - val_loss: 559.6716 - val_mae: 19.5128\n",
      "Epoch 830/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6915 - mae: 19.5796 - val_loss: 559.5090 - val_mae: 19.5005\n",
      "Epoch 831/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6656 - mae: 19.5756 - val_loss: 559.4817 - val_mae: 19.4979\n",
      "Epoch 832/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6902 - mae: 19.5747 - val_loss: 559.5275 - val_mae: 19.5022\n",
      "Epoch 833/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6888 - mae: 19.5785 - val_loss: 559.4393 - val_mae: 19.4929\n",
      "Epoch 834/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7225 - mae: 19.5757 - val_loss: 559.4637 - val_mae: 19.4959\n",
      "Epoch 835/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7007 - mae: 19.5760 - val_loss: 559.5254 - val_mae: 19.5020\n",
      "Epoch 836/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7010 - mae: 19.5718 - val_loss: 559.6392 - val_mae: 19.5107\n",
      "Epoch 837/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6801 - mae: 19.5780 - val_loss: 559.5445 - val_mae: 19.5036\n",
      "Epoch 838/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6898 - mae: 19.5795 - val_loss: 559.4886 - val_mae: 19.4985\n",
      "Epoch 839/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6729 - mae: 19.5759 - val_loss: 559.4536 - val_mae: 19.4947\n",
      "Epoch 840/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7025 - mae: 19.5742 - val_loss: 559.5048 - val_mae: 19.5001\n",
      "Epoch 841/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6505 - mae: 19.5730 - val_loss: 559.5743 - val_mae: 19.5060\n",
      "Epoch 842/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6411 - mae: 19.5822 - val_loss: 559.3927 - val_mae: 19.4843\n",
      "Epoch 843/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6952 - mae: 19.5709 - val_loss: 559.5286 - val_mae: 19.5022\n",
      "Epoch 844/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6183 - mae: 19.5813 - val_loss: 559.3872 - val_mae: 19.4817\n",
      "Epoch 845/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7040 - mae: 19.5718 - val_loss: 559.4728 - val_mae: 19.4969\n",
      "Epoch 846/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6649 - mae: 19.5768 - val_loss: 559.4268 - val_mae: 19.4912\n",
      "Epoch 847/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6501 - mae: 19.5685 - val_loss: 559.7038 - val_mae: 19.5149\n",
      "Epoch 848/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6410 - mae: 19.5856 - val_loss: 559.3888 - val_mae: 19.4826\n",
      "Epoch 849/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6453 - mae: 19.5749 - val_loss: 559.3908 - val_mae: 19.4835\n",
      "Epoch 850/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.7039 - mae: 19.5783 - val_loss: 559.3861 - val_mae: 19.4788\n",
      "Epoch 851/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7185 - mae: 19.5749 - val_loss: 559.3976 - val_mae: 19.4857\n",
      "Epoch 852/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7039 - mae: 19.5724 - val_loss: 559.4547 - val_mae: 19.4948\n",
      "Epoch 853/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6901 - mae: 19.5790 - val_loss: 559.4604 - val_mae: 19.4955\n",
      "Epoch 854/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6641 - mae: 19.5698 - val_loss: 559.5903 - val_mae: 19.5072\n",
      "Epoch 855/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6541 - mae: 19.5761 - val_loss: 559.5205 - val_mae: 19.5015\n",
      "Epoch 856/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6753 - mae: 19.5745 - val_loss: 559.5878 - val_mae: 19.5070\n",
      "Epoch 857/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6829 - mae: 19.5815 - val_loss: 559.3992 - val_mae: 19.4861\n",
      "Epoch 858/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6718 - mae: 19.5749 - val_loss: 559.3879 - val_mae: 19.4822\n",
      "Epoch 859/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.7033 - mae: 19.5729 - val_loss: 559.4527 - val_mae: 19.4946\n",
      "Epoch 860/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6415 - mae: 19.5736 - val_loss: 559.5800 - val_mae: 19.5064\n",
      "Epoch 861/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6983 - mae: 19.5765 - val_loss: 559.5289 - val_mae: 19.5023\n",
      "Epoch 862/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6942 - mae: 19.5787 - val_loss: 559.4443 - val_mae: 19.4936\n",
      "Epoch 863/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6801 - mae: 19.5724 - val_loss: 559.5679 - val_mae: 19.5055\n",
      "Epoch 864/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6743 - mae: 19.5760 - val_loss: 559.6226 - val_mae: 19.5095\n",
      "Epoch 865/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6879 - mae: 19.5771 - val_loss: 559.5194 - val_mae: 19.5014\n",
      "Epoch 866/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6778 - mae: 19.5762 - val_loss: 559.4949 - val_mae: 19.4992\n",
      "Epoch 867/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7103 - mae: 19.5754 - val_loss: 559.5438 - val_mae: 19.5035\n",
      "Epoch 868/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7018 - mae: 19.5794 - val_loss: 559.4290 - val_mae: 19.4915\n",
      "Epoch 869/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6439 - mae: 19.5773 - val_loss: 559.3927 - val_mae: 19.4843\n",
      "Epoch 870/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.5736 - mae: 19.5615 - val_loss: 559.8171 - val_mae: 19.5217\n",
      "Epoch 871/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6948 - mae: 19.5769 - val_loss: 559.7001 - val_mae: 19.5146\n",
      "Epoch 872/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6995 - mae: 19.5800 - val_loss: 559.4949 - val_mae: 19.4991\n",
      "Epoch 873/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6737 - mae: 19.5773 - val_loss: 559.4241 - val_mae: 19.4908\n",
      "Epoch 874/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6677 - mae: 19.5714 - val_loss: 559.5303 - val_mae: 19.5024\n",
      "Epoch 875/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7000 - mae: 19.5740 - val_loss: 559.5774 - val_mae: 19.5062\n",
      "Epoch 876/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6475 - mae: 19.5740 - val_loss: 559.7195 - val_mae: 19.5159\n",
      "Epoch 877/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7087 - mae: 19.5821 - val_loss: 559.4321 - val_mae: 19.4919\n",
      "Epoch 878/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6689 - mae: 19.5723 - val_loss: 559.5651 - val_mae: 19.5053\n",
      "Epoch 879/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6790 - mae: 19.5774 - val_loss: 559.4200 - val_mae: 19.4902\n",
      "Epoch 880/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6260 - mae: 19.5686 - val_loss: 559.6407 - val_mae: 19.5107\n",
      "Epoch 881/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6701 - mae: 19.5816 - val_loss: 559.4415 - val_mae: 19.4932\n",
      "Epoch 882/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6596 - mae: 19.5762 - val_loss: 559.4117 - val_mae: 19.4887\n",
      "Epoch 883/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6951 - mae: 19.5736 - val_loss: 559.4508 - val_mae: 19.4944\n",
      "Epoch 884/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6674 - mae: 19.5771 - val_loss: 559.4011 - val_mae: 19.4866\n",
      "Epoch 885/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6717 - mae: 19.5711 - val_loss: 559.5638 - val_mae: 19.5051\n",
      "Epoch 886/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6893 - mae: 19.5750 - val_loss: 559.5652 - val_mae: 19.5053\n",
      "Epoch 887/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6479 - mae: 19.5825 - val_loss: 559.3906 - val_mae: 19.4835\n",
      "Epoch 888/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.7314 - mae: 19.5750 - val_loss: 559.4055 - val_mae: 19.4876\n",
      "Epoch 889/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6616 - mae: 19.5757 - val_loss: 559.3869 - val_mae: 19.4812\n",
      "Epoch 890/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6733 - mae: 19.5692 - val_loss: 559.5311 - val_mae: 19.5025\n",
      "Epoch 891/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7070 - mae: 19.5776 - val_loss: 559.5153 - val_mae: 19.5011\n",
      "Epoch 892/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6407 - mae: 19.5787 - val_loss: 559.3986 - val_mae: 19.4859\n",
      "Epoch 893/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6729 - mae: 19.5703 - val_loss: 559.4741 - val_mae: 19.4970\n",
      "Epoch 894/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6757 - mae: 19.5775 - val_loss: 559.4429 - val_mae: 19.4934\n",
      "Epoch 895/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6534 - mae: 19.5717 - val_loss: 559.7109 - val_mae: 19.5153\n",
      "Epoch 896/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6699 - mae: 19.5810 - val_loss: 559.4535 - val_mae: 19.4947\n",
      "Epoch 897/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6523 - mae: 19.5725 - val_loss: 559.5289 - val_mae: 19.5023\n",
      "Epoch 898/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6704 - mae: 19.5779 - val_loss: 559.4465 - val_mae: 19.4939\n",
      "Epoch 899/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6279 - mae: 19.5697 - val_loss: 559.7448 - val_mae: 19.5174\n",
      "Epoch 900/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7194 - mae: 19.5804 - val_loss: 559.4521 - val_mae: 19.4945\n",
      "Epoch 901/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6625 - mae: 19.5742 - val_loss: 559.6350 - val_mae: 19.5104\n",
      "Epoch 902/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6776 - mae: 19.5831 - val_loss: 559.3929 - val_mae: 19.4842\n",
      "Epoch 903/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6572 - mae: 19.5727 - val_loss: 559.5093 - val_mae: 19.5005\n",
      "Epoch 904/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 569.6751 - mae: 19.5726 - val_loss: 559.6406 - val_mae: 19.5108\n",
      "Epoch 905/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6902 - mae: 19.5800 - val_loss: 559.4921 - val_mae: 19.4988\n",
      "Epoch 906/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6754 - mae: 19.5774 - val_loss: 559.4284 - val_mae: 19.4914\n",
      "Epoch 907/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6588 - mae: 19.5735 - val_loss: 559.4667 - val_mae: 19.4962\n",
      "Epoch 908/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6549 - mae: 19.5773 - val_loss: 559.4084 - val_mae: 19.4882\n",
      "Epoch 909/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6445 - mae: 19.5789 - val_loss: 559.3863 - val_mae: 19.4805\n",
      "Epoch 910/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6985 - mae: 19.5724 - val_loss: 559.4103 - val_mae: 19.4884\n",
      "Epoch 911/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7032 - mae: 19.5714 - val_loss: 559.5180 - val_mae: 19.5013\n",
      "Epoch 912/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6843 - mae: 19.5792 - val_loss: 559.4444 - val_mae: 19.4936\n",
      "Epoch 913/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6694 - mae: 19.5763 - val_loss: 559.4229 - val_mae: 19.4906\n",
      "Epoch 914/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6827 - mae: 19.5759 - val_loss: 559.4498 - val_mae: 19.4942\n",
      "Epoch 915/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6571 - mae: 19.5702 - val_loss: 559.7615 - val_mae: 19.5184\n",
      "Epoch 916/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7033 - mae: 19.5819 - val_loss: 559.4868 - val_mae: 19.4983\n",
      "Epoch 917/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6328 - mae: 19.5783 - val_loss: 559.3998 - val_mae: 19.4862\n",
      "Epoch 918/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6351 - mae: 19.5756 - val_loss: 559.3869 - val_mae: 19.4813\n",
      "Epoch 919/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6949 - mae: 19.5724 - val_loss: 559.5167 - val_mae: 19.5012\n",
      "Epoch 920/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6859 - mae: 19.5728 - val_loss: 559.5870 - val_mae: 19.5069\n",
      "Epoch 921/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6962 - mae: 19.5783 - val_loss: 559.5743 - val_mae: 19.5060\n",
      "Epoch 922/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6835 - mae: 19.5766 - val_loss: 559.5469 - val_mae: 19.5038\n",
      "Epoch 923/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6068 - mae: 19.5699 - val_loss: 559.7965 - val_mae: 19.5205\n",
      "Epoch 924/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6310 - mae: 19.5862 - val_loss: 559.3859 - val_mae: 19.4796\n",
      "Epoch 925/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6970 - mae: 19.5731 - val_loss: 559.4435 - val_mae: 19.4935\n",
      "Epoch 926/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6682 - mae: 19.5766 - val_loss: 559.4036 - val_mae: 19.4872\n",
      "Epoch 927/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7049 - mae: 19.5753 - val_loss: 559.4302 - val_mae: 19.4917\n",
      "Epoch 928/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6564 - mae: 19.5744 - val_loss: 559.4074 - val_mae: 19.4879\n",
      "Epoch 929/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6784 - mae: 19.5731 - val_loss: 559.4840 - val_mae: 19.4981\n",
      "Epoch 930/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6851 - mae: 19.5726 - val_loss: 559.5543 - val_mae: 19.5044\n",
      "Epoch 931/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6838 - mae: 19.5771 - val_loss: 559.5176 - val_mae: 19.5013\n",
      "Epoch 932/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 569.6335 - mae: 19.5811 - val_loss: 559.3881 - val_mae: 19.4823\n",
      "Epoch 933/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6832 - mae: 19.5708 - val_loss: 559.5198 - val_mae: 19.5015\n",
      "Epoch 934/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6854 - mae: 19.5803 - val_loss: 559.4274 - val_mae: 19.4913\n",
      "Epoch 935/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6540 - mae: 19.5744 - val_loss: 559.4170 - val_mae: 19.4897\n",
      "Epoch 936/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6618 - mae: 19.5701 - val_loss: 559.5778 - val_mae: 19.5063\n",
      "Epoch 937/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6082 - mae: 19.5824 - val_loss: 559.3903 - val_mae: 19.4762\n",
      "Epoch 938/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6934 - mae: 19.5730 - val_loss: 559.3957 - val_mae: 19.4852\n",
      "Epoch 939/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7104 - mae: 19.5757 - val_loss: 559.4315 - val_mae: 19.4919\n",
      "Epoch 940/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6778 - mae: 19.5725 - val_loss: 559.5514 - val_mae: 19.5042\n",
      "Epoch 941/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6501 - mae: 19.5804 - val_loss: 559.3903 - val_mae: 19.4833\n",
      "Epoch 942/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6887 - mae: 19.5748 - val_loss: 559.4327 - val_mae: 19.4920\n",
      "Epoch 943/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6973 - mae: 19.5746 - val_loss: 559.4959 - val_mae: 19.4992\n",
      "Epoch 944/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6628 - mae: 19.5737 - val_loss: 559.5961 - val_mae: 19.5076\n",
      "Epoch 945/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6960 - mae: 19.5783 - val_loss: 559.5148 - val_mae: 19.5010\n",
      "Epoch 946/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6956 - mae: 19.5785 - val_loss: 559.4224 - val_mae: 19.4905\n",
      "Epoch 947/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6964 - mae: 19.5747 - val_loss: 559.5295 - val_mae: 19.5023\n",
      "Epoch 948/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6892 - mae: 19.5798 - val_loss: 559.4582 - val_mae: 19.4953\n",
      "Epoch 949/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6749 - mae: 19.5775 - val_loss: 559.4095 - val_mae: 19.4883\n",
      "Epoch 950/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6854 - mae: 19.5753 - val_loss: 559.4005 - val_mae: 19.4864\n",
      "Epoch 951/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7111 - mae: 19.5737 - val_loss: 559.4315 - val_mae: 19.4919\n",
      "Epoch 952/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6113 - mae: 19.5794 - val_loss: 559.3860 - val_mae: 19.4796\n",
      "Epoch 953/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7026 - mae: 19.5731 - val_loss: 559.4141 - val_mae: 19.4892\n",
      "Epoch 954/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7107 - mae: 19.5748 - val_loss: 559.4485 - val_mae: 19.4941\n",
      "Epoch 955/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6483 - mae: 19.5768 - val_loss: 559.3860 - val_mae: 19.4791\n",
      "Epoch 956/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6565 - mae: 19.5668 - val_loss: 559.6766 - val_mae: 19.5131\n",
      "Epoch 957/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7119 - mae: 19.5808 - val_loss: 559.5102 - val_mae: 19.5006\n",
      "Epoch 958/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6691 - mae: 19.5731 - val_loss: 559.5908 - val_mae: 19.5072\n",
      "Epoch 959/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6983 - mae: 19.5765 - val_loss: 559.6318 - val_mae: 19.5101\n",
      "Epoch 960/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7089 - mae: 19.5814 - val_loss: 559.4340 - val_mae: 19.4922\n",
      "Epoch 961/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6808 - mae: 19.5732 - val_loss: 559.4704 - val_mae: 19.4966\n",
      "Epoch 962/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 569.6646 - mae: 19.5726 - val_loss: 559.5646 - val_mae: 19.5052\n",
      "Epoch 963/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.6351 - mae: 19.5790 - val_loss: 559.4181 - val_mae: 19.4899\n",
      "Epoch 964/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6522 - mae: 19.5759 - val_loss: 559.4702 - val_mae: 19.4966\n",
      "Epoch 965/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6594 - mae: 19.5729 - val_loss: 559.6027 - val_mae: 19.5081\n",
      "Epoch 966/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6156 - mae: 19.5816 - val_loss: 559.4094 - val_mae: 19.4883\n",
      "Epoch 967/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6796 - mae: 19.5715 - val_loss: 559.5102 - val_mae: 19.5006\n",
      "Epoch 968/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6791 - mae: 19.5772 - val_loss: 559.4773 - val_mae: 19.4974\n",
      "Epoch 969/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6505 - mae: 19.5717 - val_loss: 559.6097 - val_mae: 19.5086\n",
      "Epoch 970/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6770 - mae: 19.5756 - val_loss: 559.5684 - val_mae: 19.5055\n",
      "Epoch 971/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6843 - mae: 19.5802 - val_loss: 559.4611 - val_mae: 19.4956\n",
      "Epoch 972/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.7015 - mae: 19.5752 - val_loss: 559.5068 - val_mae: 19.5003\n",
      "Epoch 973/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6686 - mae: 19.5745 - val_loss: 559.5054 - val_mae: 19.5002\n",
      "Epoch 974/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6578 - mae: 19.5766 - val_loss: 559.4633 - val_mae: 19.4958\n",
      "Epoch 975/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6520 - mae: 19.5693 - val_loss: 559.7032 - val_mae: 19.5149\n",
      "Epoch 976/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7124 - mae: 19.5798 - val_loss: 559.5205 - val_mae: 19.5015\n",
      "Epoch 977/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 569.7007 - mae: 19.5781 - val_loss: 559.4391 - val_mae: 19.4929\n",
      "Epoch 978/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.6758 - mae: 19.5727 - val_loss: 559.6284 - val_mae: 19.5099\n",
      "Epoch 979/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 569.6581 - mae: 19.5719 - val_loss: 559.6745 - val_mae: 19.5130\n",
      "Epoch 980/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 569.7125 - mae: 19.5800 - val_loss: 559.5574 - val_mae: 19.5047\n",
      "Epoch 981/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 569.6503 - mae: 19.5808 - val_loss: 559.4036 - val_mae: 19.4871\n",
      "Epoch 982/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 569.7155 - mae: 19.5731 - val_loss: 559.4653 - val_mae: 19.4961\n",
      "Epoch 983/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 569.6545 - mae: 19.5704 - val_loss: 559.4848 - val_mae: 19.4981\n",
      "Epoch 984/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6763 - mae: 19.5748 - val_loss: 559.5874 - val_mae: 19.5070\n",
      "Epoch 985/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6602 - mae: 19.5785 - val_loss: 559.4266 - val_mae: 19.4912\n",
      "Epoch 986/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6926 - mae: 19.5748 - val_loss: 559.5054 - val_mae: 19.5002\n",
      "Epoch 987/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6882 - mae: 19.5748 - val_loss: 559.5774 - val_mae: 19.5062\n",
      "Epoch 988/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6990 - mae: 19.5775 - val_loss: 559.5383 - val_mae: 19.5031\n",
      "Epoch 989/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.7161 - mae: 19.5788 - val_loss: 559.4720 - val_mae: 19.4968\n",
      "Epoch 990/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.7014 - mae: 19.5755 - val_loss: 559.4921 - val_mae: 19.4989\n",
      "Epoch 991/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6778 - mae: 19.5751 - val_loss: 559.5040 - val_mae: 19.5000\n",
      "Epoch 992/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6324 - mae: 19.5696 - val_loss: 559.7674 - val_mae: 19.5188\n",
      "Epoch 993/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.7015 - mae: 19.5804 - val_loss: 559.5457 - val_mae: 19.5037\n",
      "Epoch 994/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 569.6556 - mae: 19.5752 - val_loss: 559.6716 - val_mae: 19.5128\n",
      "Epoch 995/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 569.6636 - mae: 19.5789 - val_loss: 559.4585 - val_mae: 19.4953\n",
      "Epoch 996/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6949 - mae: 19.5725 - val_loss: 559.4984 - val_mae: 19.4995\n",
      "Epoch 997/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.7046 - mae: 19.5772 - val_loss: 559.5013 - val_mae: 19.4998\n",
      "Epoch 998/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6823 - mae: 19.5781 - val_loss: 559.4240 - val_mae: 19.4908\n",
      "Epoch 999/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 569.6767 - mae: 19.5746 - val_loss: 559.5001 - val_mae: 19.4997\n",
      "Epoch 1000/1000\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 569.6776 - mae: 19.5778 - val_loss: 559.4365 - val_mae: 19.4925\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=1000,\n",
    "    batch_size=16,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "251995fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (20000, 17)\n",
      "Columns: ['Grade10', 'Grade12', 'CPI', 'Backlogs', 'CodeforcesRating', 'CompetitiveExamRank', 'Projects', 'Internships', 'ExperienceMonths', 'CollegeTier', 'CommunicationSkill', 'Leadership', 'Branch', 'HackathonWins', 'ResearchPapers', 'OpenSourceContrib', 'PackageLPA']\n",
      "\n",
      "✅ Model training complete!\n",
      "\n",
      "📊 Model Evaluation:\n",
      "R² Score: 0.871\n",
      "MAE: 0.961\n",
      "RMSE: 1.202\n",
      "\n",
      "💾 Model saved as 'placement_package_predictor.pkl'\n",
      "\n",
      "💰 Predicted Package for Candidate: 55.29 LPA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"ML_Training_dataset_20000.xlsx\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "\n",
    "target = 'PackageLPA'\n",
    "\n",
    "numeric_cols = [\n",
    "    'Grade10', 'Grade12', 'CPI', 'Backlogs', 'CodeforcesRating',\n",
    "    'CompetitiveExamRank', 'Projects', 'Internships', 'ExperienceMonths',\n",
    "    'HackathonWins', 'ResearchPapers', 'OpenSourceContrib',\n",
    "    'CommunicationSkill', 'Leadership'\n",
    "]\n",
    "\n",
    "cat_cols = ['Branch', 'CollegeTier'] \n",
    "\n",
    "X = df[numeric_cols + cat_cols]\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"\\n✅ Model training complete!\")\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\n📊 Model Evaluation:\")\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "\n",
    "joblib.dump(pipeline, \"placement_package_predictor.pkl\")\n",
    "print(\"\\n💾 Model saved as 'placement_package_predictor.pkl'\")\n",
    "\n",
    "new_candidate = pd.DataFrame([{\n",
    "    'Grade10': 99,\n",
    "    'Grade12': 95,\n",
    "    'CPI': 8.9,\n",
    "    'Backlogs': 0,\n",
    "    'CodeforcesRating': 4191,\n",
    "    'CompetitiveExamRank': 59,\n",
    "    'Projects': 60,\n",
    "    'Internships': 0,\n",
    "    'ExperienceMonths': 8,\n",
    "    'HackathonWins': 1,\n",
    "    'ResearchPapers': 0,\n",
    "    'OpenSourceContrib': 0,\n",
    "    'CommunicationSkill': 10,\n",
    "    'Leadership': 5,\n",
    "    'Branch': 'CSE',\n",
    "    'CollegeTier': 'Tier 3'\n",
    "}])\n",
    "\n",
    "predicted_package = pipeline.predict(new_candidate)[0]\n",
    "print(f\"\\n💰 Predicted Package for Candidate: {predicted_package:.2f} LPA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20a759d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Package: 29.83 LPA\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "pipeline = joblib.load(\"placement_package_predictor.pkl\")\n",
    "\n",
    "new_candidate = pd.DataFrame([{\n",
    "    'Grade10': 90, 'Grade12': 88, 'CPI': 8.9, 'Backlogs': 0,\n",
    "    'CodeforcesRating': 1600, 'CompetitiveExamRank': 3200,\n",
    "    'Projects': 4, 'Internships': 2, 'ExperienceMonths': 6,\n",
    "    'HackathonWins': 1, 'ResearchPapers': 0, 'OpenSourceContrib': 3,\n",
    "    'CommunicationSkill': 8, 'Leadership': 7,\n",
    "    'Branch': 'CSE', 'CollegeTier': 'Tier 1'\n",
    "}])\n",
    "\n",
    "# Predict package\n",
    "predicted_package = pipeline.predict(new_candidate)[0]\n",
    "print(f\"Predicted Package: {predicted_package:.2f} LPA\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
